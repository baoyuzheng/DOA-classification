{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPi9ZYdpeuALR3GTF9UJhXH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"4lwyxdmzXxqP","executionInfo":{"status":"ok","timestamp":1611492033803,"user_tz":-330,"elapsed":1331,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import logging\n","from torchvision import models\n","import numpy as np\n","import torch.nn.functional as F\n","\n","def double_conv1(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(10,4),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(7,2),stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_conv2(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(9,3),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(7,3), padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","\n","class cnn(nn.Module):\n","    def __init__(self,img_ch=3,output_ch=1):\n","        super(cnn, self).__init__()\n","        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=(2,1), stride=2)\n","        self.down_conv_1 = double_conv1(img_ch, 64)\n","        self.down_conv_2 = double_conv2(64, 128)\n","        self.down_conv_3 = double_conv2(128, 256)\n","        self.out = nn.Conv2d(\n","            in_channels=256,\n","            out_channels=output_ch,\n","            kernel_size=1,stride=1,padding=0)\n","        self.fc1 = nn.Linear(125, 150)\n","        self.fc2 = nn.Linear(150, 170)\n","        self.fc3 = nn.Linear(170, 181)\n","    def forward(self, image):\n","        # cnn\n","        x1 = self.down_conv_1(image)\n","        x2 = self.max_pool_2x2(x1)\n","        x3 = self.down_conv_2(x2)\n","        x4 = self.max_pool_2x2(x3)\n","        x5 = self.down_conv_3(x4)\n","        x6 = self.fc1(x5)\n","        x7 = self.fc2(x6)\n","        x8 = self.fc3(x7)\n","        out = self.out(x8)\n","        return out\n","\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","    image = torch.rand(1, 3, 80, 500)\n","    en = cnn()\n","    en(image)\n","\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVLhipMkZj3F"},"source":["import scipy.io as sio\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np \n","import math\n","import pandas as pd\n","import cmath\n","\n","from collections import OrderedDict\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.autograd import Variable\n","\n","from functools import partial\n","from threading import Thread\n","from tornado import gen\n","\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysyO0ArZZnD7"},"source":["%%capture\n","!pip install wandb -qqq\n","import wandb\n","!wandb login"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjYwJzVPZtb4"},"source":["wandb.init(project=\"SNR_NS_10_50000_colab_CNN\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGI9P1OpZ9lm"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5I-zEGvKZrYd"},"source":["torch.cuda.empty_cache()\n","df1 = sio.loadmat(\"./gdrive/MyDrive/DOA/DOA1/SNR_NS_10_50000.mat\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLqonFGLaJRj"},"source":["max_r =0.3408866150170333\n","max_i =0.29720502659995574\n","max_p =0.06803637694674271\n","min_r =-0.33121108049659753\n","min_i =-0.298407779910961\n","min_p =0.08578641590154056\n","\n","class DOA_dataset(Dataset):\n","    def __init__(self, df):\n","        transp = np.transpose(df['NS_data'], (2, 0, 1))\n","        new = np.zeros((30000, 3, 80, 500))\n","        for i in range(0, transp.shape[0]):\n","            for j in range(0, transp.shape[1]):\n","                for k in range(0, transp.shape[2]):\n","                    new[i][0][j][k] = (transp[i][j][k].real - min_r)/(max_r-min_r)\n","                    new[i][1][j][k] = (transp[i][j][k].imag - min_i)/(max_i-min_i)\n","                    new[i][2][j][k] = (cmath.phase(transp[i][j][k]) - min_p)/(max_p-min_p)\n","\n","        self.x = torch.from_numpy(new)\n","        self.y = torch.from_numpy(np.asarray(df['DOA']))\n","        self.n_sample = len(self.y)\n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index]\n","    def __len__(self):\n","        return self.n_sample\n","\n","\n","dataset = DOA_dataset(df1)\n","validation_split = .1\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","print(dataset_size)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","\n","#dataloader = DataLoader(dataset=dff, batch_size=100, shuffle=True,  num_workers=2)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n","                                           sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=256,\n","                                                sampler=valid_sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rI0YrGpgXynH"},"source":["autoencoder = cnn()\n","criterion = nn.CrossEntropyLoss()\n","\n","if torch.cuda.is_available():\n","\tprint(torch.cuda.get_device_name(0))\n","\tautoencoder = autoencoder.cuda()\n","\toptimizer = optim.AdamW(autoencoder.parameters(), lr=0.00001, weight_decay=1e-5)\n","\tcriterion = criterion.cuda()\n","\n","#==========================================================================\n","def train():\n","\tfor i in range(200):\n","\n","\t\ttraining_loss = 0\n","\t\tautoencoder.train()\n","\t\ttcorrect = 0\n","\t\tttotal = 0\n","\t\tfor features, labels in train_loader:\n","\t\t\tfeatures, labels = Variable(features.cuda()), Variable(labels.cuda())\n","\t\t\toptimizer.zero_grad()\n","\t\t\tenn = autoencoder(features.float())\n","\t\t\tauto_outputs = torch.transpose(enn, 2, 3)\n","\t\t\tauto_outputs = torch.reshape(auto_outputs.cuda(), (auto_outputs.shape[0], 181, 1))\n","\t\t\tlosss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n","\t\t\tlosss.backward()\n","\t\t\toptimizer.step()\n","\t\t\ttraining_loss += losss.item()\n","   \n","      _, pred = torch.max(auto_outputs, 1)\n","\t\t\tttotal+= labels.reshape(-1).size(0)\n","\t\t\ttcorrect+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n","\n","\t\tvalidation_loss = 0\n","\t\tcorrect = 0\n","\t\ttotal = 0\n","\t\tautoencoder.eval()\n","\t\twith torch.no_grad():\n","\t\t\tfor features, label in validation_loader:\n","\t\t\t\tfeatures, label = Variable(features.cuda()), Variable(label.cuda())\n","\t\t\t\tenn = autoencoder(features.float())\n","\t\t\t\tauto_outputs = torch.transpose(enn, 2, 3)\n","\t\t\t\tauto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, 1))\n","\t\t\t\tloss = criterion(auto_outputs.cuda(), label.type(torch.LongTensor).cuda())\n","\t\t\t\tvalidation_loss += loss.item()\n","        _, pred = torch.max(auto_outputs, 1)\n","\t\t\t\ttotal+= label.reshape(-1).size(0)\n","\t\t\t\tcorrect+=(pred.reshape(-1).cuda() == label.reshape(-1)).sum().item()\n","    \n","\t\twandb.log({\"Trainig Acc\":(100*(tcorrect/ttotal)), \"Traning loss\":(training_loss/len(train_loader)), \"Validation Acc\":(100*(correct/total)), \"Validation loss\":( validation_loss/len(validation_loader))})\n","\t\tprint(\"Epoch {} - Traningloss: {}\".format(i+1, training_loss/len(train_loader)))\n","\t\tprint(\"Validationloss: {}\".format( validation_loss/len(validation_loader)))\n","\t\tprint(\"Trianing Acc: {}\".format( 100*(tcorrect/ttotal)))\n","\t\tprint(\"Validaion Acc: {}\".format( 100*(correct/total)))\n","\twandb.finish()\n","train()\n","print(\"Training Complete\")\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]}]}