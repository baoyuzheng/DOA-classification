{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5nKnVnaiWlkXHoeq1/iBz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UDJDisL90xDE"},"source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import logging\n","from torchvision import models\n","import numpy as np\n","import torch.nn.functional as F\n","\n","def double_conv1(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(9,2),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(9,2),stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_conv2(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(9,3),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(7,3), padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","\n","def up_conv1(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.ConvTranspose2d(in_c, out_c, kernel_size=(1, 19), stride=3))\n","    return conv\n","    \n","def up_conv2(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.ConvTranspose2d(in_c, out_c, kernel_size=(1,1), stride=2))\n","    return conv\n","\n","def double_upconv1(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(3,3),padding=1 , bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(3,3), padding=1 , bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_upconv2(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(1, 1) , bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(1, 1) , bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","class encoder(nn.Module):\n","    def __init__(self,img_ch=3,output_ch=3):\n","        super(encoder, self).__init__()\n","        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=(2,1), stride=2)\n","        self.down_conv_1 = double_conv1(img_ch, 64)\n","        self.down_conv_2 = double_conv2(64, 128)\n","        self.down_conv_3 = double_conv2(128, 256)\n","    \n","    def forward(self, image):\n","        # encoder\n","        x1 = self.down_conv_1(image)\n","        x2 = self.max_pool_2x2(x1)\n","        x3 = self.down_conv_2(x2)\n","        x4 = self.max_pool_2x2(x3)\n","        x5 = self.down_conv_3(x4)\n","        return x5\n","\n","class decoder(nn.Module):\n","    def __init__(self,img_ch=3,output_ch=1):\n","        super(decoder, self).__init__()\n","        self.up_trans_1 = up_conv1(256, 128)\n","        self.up_conv_1 = double_upconv1(128, 64)\n","        \n","        self.up_trans_2 = up_conv2(64, 32)\n","        self.up_conv_2 = double_upconv2(32, 16)\n","        \n","        self.out = nn.Conv2d(\n","            in_channels=16,\n","            out_channels=output_ch,\n","            kernel_size=1,stride=1,padding=0)\n","    \n","    def forward(self, image):\n","        # decoder\n","        x = self.up_trans_1(image)\n","        x = self.up_conv_1(x)\n","        x = self.up_trans_2(x)\n","        x = self.up_conv_2(x)\n","\n","        # output\n","        x = self.out(x)\n","        return x\n","\n","\n","\n","# model = models.vgg16()\n","\n","from collections import OrderedDict\n","\n","if __name__ == \"__main__\":\n","    image = torch.rand(1, 3, 80, 100)\n","    en = encoder()\n","    de = decoder()\n","    de(en(image))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LzBNh1a1Jd5"},"source":["import scipy.io as sio\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np \n","import math\n","import pandas as pd\n","import cmath\n","\n","\n","from collections import OrderedDict\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.autograd import Variable\n","#==========================================================================\n","# For Plotting loss graph\n","# Bokeh\n","from bokeh.io import curdoc\n","from bokeh.layouts import column\n","from bokeh.models import ColumnDataSource\n","from bokeh.plotting import figure\n","\n","from functools import partial\n","from threading import Thread\n","from tornado import gen\n","\n","torch.cuda.empty_cache()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXd3Fxt11esb","executionInfo":{"status":"ok","timestamp":1611490616808,"user_tz":-330,"elapsed":1302,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}},"outputId":"caf78b58-e635-425e-89ef-c3f4e8d58356"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eA9KeN_i1Jia"},"source":["#==========================================================================\n","df1 = sio.loadmat(\"./gdrive/MyDrive/DOA/DOA1/SNR_NS_10_50000.mat\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slsWHTZyUZC-"},"source":["\n","max_r =0.3408866150170333\n","max_i =0.29720502659995574\n","max_p =0.06803637694674271\n","min_r =-0.33121108049659753\n","min_i =-0.298407779910961\n","min_p =0.08578641590154056\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jzvW2KH1Jm5","executionInfo":{"status":"ok","timestamp":1611490839269,"user_tz":-330,"elapsed":184839,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}},"outputId":"d1c4eff3-5aed-4b98-81d2-f79e47df1cfd"},"source":["class DOA_dataset(Dataset):\n","    def __init__(self, df):\n","        transp = np.transpose(df['NS_data'], (2, 0, 1))\n","        new = np.zeros((50000, 3, 80, 100))\n","        for i in range(0, transp.shape[0]):\n","            for j in range(0, transp.shape[1]):\n","                for k in range(0, transp.shape[2]):\n","                    new[i][0][j][k] = (transp[i][j][k].real - min_r)/(max_r-min_r)\n","                    new[i][1][j][k] = (transp[i][j][k].imag - min_i)/(max_i-min_i)\n","                    new[i][2][j][k] = (cmath.phase(transp[i][j][k]) - min_p)/(max_p-min_p)\n","\n","        self.x = torch.from_numpy(new)\n","        self.y = torch.from_numpy(np.asarray(df['DOA']))\n","        self.n_sample = len(self.y)\n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index]\n","    def __len__(self):\n","        return self.n_sample\n","\n","\n","dataset = DOA_dataset(df1)\n","validation_split = 0.1\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","print(len(train_indices))\n","print(len(val_indices))\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, \n","                                           sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=256,\n","                                                sampler=valid_sampler)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["45000\n","5000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kj_kEJpy09_6"},"source":["%%capture\n","!pip install wandb -qqq\n","import wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UFZRVunwLfi","executionInfo":{"status":"ok","timestamp":1611490865141,"user_tz":-330,"elapsed":5881,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}},"outputId":"5b409da0-452a-4ccf-f3ce-c8ab6cf3b38c"},"source":["!wandb login"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmadkhan242\u001b[0m (use `wandb login --relogin` to force relogin)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zE10SxsITaSJ"},"source":["wandb.init(project=\"SNR_NS_10_50000_colab\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6DChVT4wLsy","executionInfo":{"status":"ok","timestamp":1611492103814,"user_tz":-330,"elapsed":1774,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a99PwO5_1RO0","outputId":"e6b0527e-0510-4bda-a191-c9564b9b8e89"},"source":["en = encoder()\n","\n","# pre_model = torch.load(\"./gdrive/MyDrive/DOA/DOA1/Att_DOA1.pth\")\n","\n","# en.load_state_dict(pre_model, strict=False)\n","de_model = decoder()\n","\n","autoencoder = nn.Sequential(en, de_model)\n","\n","#==========================================================================\n","\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","if torch.cuda.is_available():\n","  print(torch.cuda.get_device_name(0))\n","  autoencoder = autoencoder.cuda()\n","  optimizer = optim.Adam(autoencoder.parameters(), lr=0.00001)\n","  criterion = criterion.cuda()\n","\n","\n","#==========================================================================\n","\n","train_loss_list = []\n","val_loss_list = []\n","epc_list = []\n","def train():\n","  for i in range(50):\n","\n","    training_loss = 0\n","    autoencoder.train()\n","    tcorrect = 0\n","    ttotal = 0\n","    for features, labels in train_loader:\n","      features, labels = Variable(features.cuda()), Variable(labels.cuda())\n","      optimizer.zero_grad()\n","      enn = autoencoder(features.float())\n","      auto_outputs = torch.transpose(enn, 2, 3)\n","      auto_outputs = torch.reshape(auto_outputs.cuda(), (auto_outputs.shape[0], 181, 1))\n","      losss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n","      losss.backward()\n","      optimizer.step()\n","      training_loss += losss.item()\n","\n","      _, pred = torch.max(auto_outputs, 1)\n","      ttotal+= labels.reshape(-1).size(0)\n","      tcorrect+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n","\n","    validation_loss = 0\n","    correct = 0\n","    total = 0\n","    autoencoder.eval()\n","    with torch.no_grad():\n","      for features, labels in validation_loader:\n","        features, labels = Variable(features.cuda()), Variable(labels.cuda())\n","\n","        enn = autoencoder(features.float())\n","        auto_outputs = torch.transpose(enn, 2, 3)\n","        auto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, 1))\n","        loss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n","\n","        _, pred = torch.max(auto_outputs, 1)\n","        total+= labels.size(0)\n","        correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n","        validation_loss += loss.item()\n","    train_loss_list.append(training_loss/len(train_loader))\n","    val_loss_list.append(validation_loss/len(validation_loader))\n","    epc_list.append(i)\n","    wandb.log({\"Trainig Acc\":(100*(tcorrect/ttotal)), \"Traning loss\":(training_loss/len(train_loader)), \"Validation Acc\":(100*(correct/total)), \"Validation loss\":( validation_loss/len(validation_loader))})\n","    print(\"Epoch {} - Traningloss: {}\".format(i+1, training_loss/len(train_loader)))\n","    print(\"Validationloss: {}\".format( validation_loss/len(validation_loader)))\n","    print(\"Trianing Acc: {}\".format( 100*(tcorrect/ttotal)))\n","    print(\"Validaion Acc: {}\".format( 100*(correct/total)))\n","\n","train()\n","\n","wandb.finish()\n","print(\"Training Complete\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P8mTWSTsAZKj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tfm6sWLZ1RSE","executionInfo":{"status":"ok","timestamp":1611492092441,"user_tz":-330,"elapsed":1985,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}}},"source":["\n","wandb.finish()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wetc6opbfSca","executionInfo":{"status":"ok","timestamp":1611492086480,"user_tz":-330,"elapsed":2178,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JN2ndeX1RVP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFBvbD_N1RYy"},"source":[""],"execution_count":null,"outputs":[]}]}