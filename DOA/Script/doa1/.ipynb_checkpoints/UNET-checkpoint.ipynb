{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sealed-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "def single_conv(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return conv\n",
    "\n",
    "def double_conv1(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=(3,2),padding=1 ,stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=(3,2),stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return conv\n",
    "\n",
    "def double_conv2(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=(3,3),padding=1 ,stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=(3,3), padding=1 ,stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return conv\n",
    "\n",
    "\n",
    "def up_conv1(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_c, out_c, kernel_size=(2, 2), stride=2))\n",
    "    return conv\n",
    "    \n",
    "def up_conv2(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_c, out_c, kernel_size=(2,2), stride=2))\n",
    "    return conv\n",
    "\n",
    "\n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "\n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RRCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x+x1\n",
    "\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n",
    "\n",
    "\n",
    "class Att_R2U(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=3,t=2):\n",
    "        super(Att_R2U, self).__init__()\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.RCNN1 = RRCNN_block(img_ch, 64, t=t)\n",
    "        self.RCNN2 = RRCNN_block(64, 128, t=t)\n",
    "        self.RCNN3 = RRCNN_block(128, 256, t=t)\n",
    "\n",
    "        self.up_trans_1 = up_conv1(256, 128)\n",
    "        self.Att1 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_RRCNN1 = RRCNN_block(256, 128,t=t)\n",
    "        \n",
    "        self.up_trans_2 = up_conv2(128, 64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_RRCNN2 = RRCNN_block(128, 64,t=t)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=output_ch,\n",
    "            kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # encoder\n",
    "        # print(\"Input Image            => \", image.size())\n",
    "        # print(\"Encoder =================\")\n",
    "        x1 = self.RCNN1(image)\n",
    "        # print(\"Conv3x2, S1, P1        => \", x1.size())\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        # print(\"max_pool_2x1           => \", x2.size())\n",
    "        x3 = self.RCNN2(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "        # print(\"Conv3x3, S1, P1        => \", x3.size())\n",
    "        x4 = self.max_pool_2x2(x3)\n",
    "        # print(\"max_pool_2x1           => \", x4.size())\n",
    "        x5 = self.RCNN3(x4)\n",
    "        x5 = self.dropout(x5)\n",
    "        # print(\"Conv3x3, S1, P1        => \", x5.size())\n",
    "        \n",
    "        \n",
    "        # decoder\n",
    "        # print(\"Decoder =================\")\n",
    "        x = self.up_trans_1(x5)\n",
    "        # print(\"up_trans_1x18, S3, P0  => \", x.size())\n",
    "        x3 = nn.functional.interpolate(x3, (x.size()[2], x.size()[3]))\n",
    "        x3 = self.Att1(g=x,x=x3)\n",
    "        x = self.Up_RRCNN1(torch.cat([x, x3], 1))\n",
    "        x = self.dropout(x)\n",
    "        # print(\"up_conv_3x3, S1, P1    => \", x.size())\n",
    "\n",
    "        x = self.up_trans_2(x)\n",
    "        # print(\"up_trans_2x2, S2, P0   => \", x.size())\n",
    "        x1 = nn.functional.interpolate(x1, (x.size()[2], x.size()[3]))\n",
    "        x1 = self.Att2(g=x,x=x1)\n",
    "        x = self.Up_RRCNN2(torch.cat([x, x1], 1))\n",
    "        x = self.dropout(x)\n",
    "        # print(\"up_conv_2x3, s1, p1    => \", x.size())\n",
    "        # output\n",
    "        x = self.out(x)\n",
    "#         print(x.size())\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# #     print(\"start\")\n",
    "#     image = torch.rand(1, 3, 8, 100)\n",
    "#     model = Att_R2U()\n",
    "#     model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "breeding-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "\n",
    "#from unet import UNet\n",
    "# from auto import encoder, decoder\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "#==========================================================================\n",
    "# For Plotting loss graph\n",
    "# Bokeh\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from functools import partial\n",
    "from threading import Thread\n",
    "from tornado import gen\n",
    "# from AttRCNN_UNet import Att_R2U\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "import sys\n",
    "# from dataloader import norm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sys import getsizeof\n",
    "# import wandb\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "common-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_list = []\n",
    "test_dataset_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "proved-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_00_3.mat\")\n",
    "df2  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_10_3.mat\")\n",
    "df3  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_20_3.mat\")\n",
    "df4  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_30_3.mat\")\n",
    "df5  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_40_3.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "previous-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [df1, df2, df3, df4, df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "driving-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10, 100000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df1[\"SNS_data\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cross-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset_list = []\n",
    "new_test_dataset_list = []\n",
    "train_dataset_list_label = []\n",
    "test_dataset_list_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "formed-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    data = np.transpose(df['SNS_data'], (2, 0, 1))\n",
    "    label = df['DOA']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.15, random_state=42)\n",
    "    new_train_dataset_list.extend(X_train)\n",
    "    new_test_dataset_list.append(X_test.tolist())\n",
    "    train_dataset_list_label.extend(y_train)\n",
    "    test_dataset_list_label.append(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unexpected-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in df:\n",
    "    create_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "qualified-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del df4\n",
    "del df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fabulous-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset_list_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db0a605-0ab4-47e4-9e74-bc12920e7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(new_train_dataset_list):\n",
    "    new = np.zeros((3, 8, 10))\n",
    "    for j in range(0, data.shape[0]):\n",
    "        for k in range(0, data.shape[1]):\n",
    "            new[0][j][k] = data[j][k].real\n",
    "            new[1][j][k] = data[j][k].imag\n",
    "            new[2][j][k] = cmath.phase(data[j][k])\n",
    "    new_train_dataset_list[idx] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47db8d1e-362e-469a-b9da-3827588119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(new_test_dataset_list):\n",
    "    for i, ndata in enumerate(data):\n",
    "        new = np.zeros((3, 8, 10))\n",
    "        for j in range(0, 8):\n",
    "            for k in range(0, 10):\n",
    "                new[0][j][k] = ndata[j][k].real\n",
    "                new[1][j][k] = ndata[j][k].imag\n",
    "                new[2][j][k] = cmath.phase(ndata[j][k])\n",
    "        new_test_dataset_list[idx][i] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a962c9cc-b765-40f8-8c68-fc1b8d27178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, test, batch_size, train_bool=True):\n",
    "    class DOA_dataset(Dataset):\n",
    "        def __init__(self, train, test):\n",
    "            self.x = torch.from_numpy(np.array(train))\n",
    "            self.y = torch.from_numpy(np.asarray(test))\n",
    "            self.n_sample = len(self.y)\n",
    "        def __getitem__(self, index):\n",
    "            return self.x[index], self.y[index]\n",
    "        def __len__(self):\n",
    "            return self.n_sample\n",
    "\n",
    "\n",
    "    dataset = DOA_dataset(train, test)\n",
    "    \n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_bool)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10d8f6b3-5ed0-4fb0-a690-3b63a0b463db",
   "metadata": {},
   "outputs": [],
   "source": [
    "doa2_train_loader = get_data(new_train_dataset_list, train_dataset_list_label, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23363836-0597-40a6-9e2d-490bf17a8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_train_dataset_list\n",
    "del train_dataset_list_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340bcf9d-3c26-45e8-8ddf-eccf33fb19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57058d96-7dbb-4945-a778-5cb4b50e41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(new_test_dataset_list):\n",
    "    print(type(data))\n",
    "    f = get_data(data, test_dataset_list_label[idx], 128, False)\n",
    "    test_dataset_list.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb01ccf-99f5-4c5f-a431-0ae4ddc4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_test_dataset_list\n",
    "del new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ecf8845-dc32-4ac2-9059-4b27cd66af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "# from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "# distance = distances.CosineSimilarity()\n",
    "# reducer = reducers.ThresholdReducer(low = 0)\n",
    "# loss_func = losses.TripletMarginLoss(margin = 0.2, distance = distance, reducer = reducer)\n",
    "# mining_func = miners.TripletMarginMiner(margin = 0.2, distance = distance, type_of_triplets = \"semihard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ac8e4e8-af2d-4c2a-93a5-f20f09d4c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "#         data, labels = Variable(data.cuda()), Variable(labels.cuda())\n",
    "#         optimizer.zero_grad()\n",
    "# #         image = torch.rand(1, 3, 8, 100)\n",
    "#         embeddings = model(data.float())\n",
    "# #         embeddings = embeddings.reshape(embeddings.size(0), 181)\n",
    "# #         labels = torch.zeros([64, 1]).cuda()\n",
    "#         print(embeddings.size(), data.size())\n",
    "#         indices_tuple = mining_func(embeddings,data.float())\n",
    "#         loss = loss_func(embeddings, labels, indices_tuple)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 1400 == 0:\n",
    "#             print(\"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(epoch, batch_idx, loss, mining_func.num_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee4cb4c1-ff39-4bdd-a2cb-39db667ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### convenient function from pytorch-metric-learning ###\n",
    "# def get_all_embeddings(dataset, model):\n",
    "#     tester = testers.BaseTester()\n",
    "#     return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "# # def test(testloader, model):\n",
    "# #     model.eval()\n",
    "# #     validation_loss = 0\n",
    "# #     correct = 0\n",
    "# #     total = 0\n",
    "# #     with torch.no_grad():\n",
    "# #         for features, labels in testloader:\n",
    "# #             features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "# #             enn = autoencoder(features.float())\n",
    "# #             auto_outputs = torch.transpose(enn, 2, 3)\n",
    "# #             auto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, 1))\n",
    "# #             loss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
    "\n",
    "# #             _, pred = torch.max(auto_outputs, 1)\n",
    "# #             total+= labels.reshape(-1).size(0)\n",
    "# #             correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "# #             validation_loss += loss.item()\n",
    "# # #             wandb.log({\"Validation Acc \"+str(val_data):(100*(correct/total)),\"Validation Loss \"+str(val_data):( validation_loss/len(test_dataset_list[val_data]))})\n",
    "# #     print(val_data*10, \"dB SNR is validated\")\n",
    "# # #     vl1[val_data].append((100*(correct/total)))\n",
    "# #     print(\"Validationloss: {}\".format( validation_loss/len(test_dataset_list[val_data])), \" ---- Validation Acc: {}\".format(100*(correct/total)))\n",
    "# # print(\"\\n\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6faa8f4b-5d81-45a9-8a52-2a7e35355ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# device = torch.device(\"cuda\")\n",
    "# autoencoder = Att_R2U()\n",
    "# model = autoencoder.cuda()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ### pytorch-metric-learning stuff ###\n",
    "# distance = distances.CosineSimilarity()\n",
    "# reducer = reducers.ThresholdReducer(low = 0)\n",
    "# loss_func = losses.TripletMarginLoss(margin = 0.2, distance = distance, reducer = reducer) #losses.NTXentLoss(temperature=0.07) \n",
    "# mining_func = miners.TripletMarginMiner(margin = 0.2, distance = distance, type_of_triplets = \"semihard\")\n",
    "# accuracy_calculator = AccuracyCalculator(include = (\"precision_at_1\",), k = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4b0e0e4-b736-40e8-9c00-fd3c4efde593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(0, 100):\n",
    "# #     for data in range(0, len(train_dataset_list)):\n",
    "# #         print(\"Training has started for \", str(data*10), \"dB SNR\" )\n",
    "#     train(model, loss_func, mining_func, device, doa2_train_loader, optimizer, epoch+1)\n",
    "# #     for val_data in range(0, len(test_dataset_list)):\n",
    "# #         print(\"Validation has started for \", str(val_data*10), \"dB SNR\" )\n",
    "# #         test(train_dataset_list[val_data], test_dataset_list[val_data], model, accuracy_calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13b8d2-9fdd-473e-a342-cd003f1ed4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Att_R2U()\n",
    "\n",
    "num_epochs = 50\n",
    "doa = 100\n",
    "weights_dir = \"./doa_weights/\"\n",
    "\n",
    "autoencoder = ResNet18()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if ('SNS_DOA_{}_model.pth'.format(doa) in [f for f in listdir(weights_dir) if isfile(join(weights_dir, f))]):\n",
    "    print(\"Pre-trained available for DOA_{}_model.pth\".format(doa))\n",
    "    autoencoder = torch.load(os.path.join(weights_dir, 'SNS_DOA_{}_model.pth'.format(doa)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(torch.cuda.get_device_name(0))\n",
    "\tclassification_model = autoencoder.cuda()\n",
    "\toptimizer = optim.AdamW(classification_model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\tcriterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa4a19-7e9a-4c9a-84b6-21109d5ed746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
