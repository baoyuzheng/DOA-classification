{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78992114-5486-4c23-8890-f7553b306f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "\n",
    "#from unet import UNet\n",
    "# from auto import encoder, decoder\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "#==========================================================================\n",
    "# For Plotting loss graph\n",
    "# Bokeh\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from functools import partial\n",
    "from threading import Thread\n",
    "from tornado import gen\n",
    "# from AttRCNN_UNet import Att_R2U\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "import sys\n",
    "# from dataloader import norm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sys import getsizeof\n",
    "# import wandb\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b70b09-f37a-496c-92ca-a245d1ca3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_type = \"SNS\" # Sub-Nyquist or Nyquist sample\n",
    "no_samples = 100\n",
    "no_doa = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8838ec5c-b33c-4366-a3b0-4c419c5411a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dataset_path =  (\"../../SNS_dataset/300/DOA2\")\n",
    "root_dataset_path =  (\"../../{}_dataset/{}/DOA{}\".format(nq_type, no_samples, no_doa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26fac427-f7f1-4614-9842-051579a103f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, dataset_type):\n",
    "  \n",
    "    data = np.transpose(df[dataset_type], (2, 0, 1))\n",
    "    label = df['DOA']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.15, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842d12d9-e9f4-4ed3-91a3-fc4afc9935fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, test, batch_size, train_bool=True):\n",
    "    class DOA_dataset(Dataset):\n",
    "        def __init__(self, train, test):\n",
    "            self.x = torch.from_numpy(np.array(train))\n",
    "            self.y = torch.from_numpy(np.asarray(test))\n",
    "            self.n_sample = len(self.y)\n",
    "        def __getitem__(self, index):\n",
    "            return self.x[index], self.y[index]\n",
    "        def __len__(self):\n",
    "            return self.n_sample\n",
    "\n",
    "\n",
    "    dataset = DOA_dataset(train, test)\n",
    "    \n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_bool)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43265276-b10e-446d-a9e5-1a9f28d0424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(root_path, dataset_type, ss):\n",
    "    # Loading all dataset \n",
    "    logging.info('Data prepration started')\n",
    "#     df1  = sio.loadmat(root_path + \"/SNR_{}_00_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "#     df2  = sio.loadmat(root_path + \"/SNR_{}_10_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "#     df3  = sio.loadmat(root_path + \"/SNR_{}_20_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "#     df4  = sio.loadmat(root_path + \"/SNR_{}_30_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "#     df5  = sio.loadmat(root_path + \"/SNR_{}_40_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "    df1  = sio.loadmat(\"../SNS_dataset/100/DOA4/SNR_SNS_00_4.mat\")\n",
    "    df2  = sio.loadmat(\"../SNS_dataset/100/DOA4/SNR_SNS_10_4.mat\")\n",
    "    df3  = sio.loadmat(\"../SNS_dataset/100/DOA4/SNR_SNS_20_4.mat\")\n",
    "    df4  = sio.loadmat(\"../SNS_dataset/100/DOA4/SNR_SNS_30_4.mat\")\n",
    "    df5  = sio.loadmat(\"../SNS_dataset/100/DOA4/SNR_SNS_40_4.mat\")\n",
    "    df = [df1, df2, df3, df4, df5]\n",
    "\n",
    "    new_train_dataset_list = []\n",
    "    new_test_dataset_list = []\n",
    "    train_dataset_list_label = []\n",
    "    test_dataset_list_label = []\n",
    "\n",
    "    dataset_len = 0\n",
    "    if dataset_type == \"SNS\":\n",
    "        dataset_len = ss//10\n",
    "    if dataset_type == \"NS\":\n",
    "        dataset_len = ss\n",
    "\n",
    "    for file in df:\n",
    "        X_train, X_test, y_train, y_test = create_dataset(file, dataset_type+\"_data\")\n",
    "        new_train_dataset_list.extend(X_train)\n",
    "        new_test_dataset_list.append(X_test.tolist())\n",
    "        train_dataset_list_label.extend(y_train)\n",
    "        test_dataset_list_label.append(y_test.tolist())\n",
    "\n",
    "    # Deleteing all unused memory\n",
    "    del df\n",
    "\n",
    "    # Dividing into different channels\n",
    "    logging.info('Dividing your dataset into 3 channel')\n",
    "\n",
    "    for idx, data in enumerate(new_train_dataset_list):\n",
    "        new = np.zeros((3, 8, dataset_len))\n",
    "        for j in range(0, data.shape[0]):\n",
    "            for k in range(0, data.shape[1]):\n",
    "                new[0][j][k] = data[j][k].real\n",
    "                new[1][j][k] = data[j][k].imag\n",
    "                new[2][j][k] = cmath.phase(data[j][k])\n",
    "        new_train_dataset_list[idx] = new\n",
    "\n",
    "    for idx, data in enumerate(new_test_dataset_list):\n",
    "        for i, ndata in enumerate(data):\n",
    "            new = np.zeros((3, 8, dataset_len))\n",
    "            for j in range(0, 8):\n",
    "                for k in range(0, dataset_len):\n",
    "                    new[0][j][k] = ndata[j][k].real\n",
    "                    new[1][j][k] = ndata[j][k].imag\n",
    "                    new[2][j][k] = cmath.phase(ndata[j][k])\n",
    "            new_test_dataset_list[idx][i] = new\n",
    "\n",
    "    # Generaring train loader\n",
    "    logging.info('Creating Train dataloader')\n",
    "    doa_train_loader = get_data(new_train_dataset_list, train_dataset_list_label, 64, True)\n",
    "\n",
    "    # Deleteing all unused memory\n",
    "    del new_train_dataset_list\n",
    "    del train_dataset_list_label\n",
    "\n",
    "    test_dataset_list = []\n",
    "    logging.info('Creating Validation dataloader')\n",
    "    for idx, data in enumerate(new_test_dataset_list):\n",
    "        f = get_data(data, test_dataset_list_label[idx], 128, False)\n",
    "        test_dataset_list.append(f)\n",
    "\n",
    "    # Deleteing all unused memory\n",
    "    del new_test_dataset_list\n",
    "    del new\n",
    "    logging.info('Your dataset is ready !!')\n",
    "    return doa_train_loader ,test_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae768477-b14b-4fff-ba15-0b698dc5bc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-30 05:36:35,496 - Data prepration started\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../SNS_dataset/100/DOA4/SNR_SNS_00_4.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../SNS_dataset/100/DOA4/SNR_SNS_00_4.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-da99ed361322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoa_train_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SNS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-1fef79bc8d6b>\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(root_path, dataset_type, ss)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     df4  = sio.loadmat(root_path + \"/SNR_{}_30_{}_{}.mat\".format(nq_type, no_doa, no_samples))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     df5  = sio.loadmat(root_path + \"/SNR_{}_40_{}_{}.mat\".format(nq_type, no_doa, no_samples))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../SNS_dataset/100/DOA4/SNR_SNS_00_4.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../SNS_dataset/100/DOA4/SNR_SNS_10_4.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf3\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../SNS_dataset/100/DOA4/SNR_SNS_20_4.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../SNS_dataset/100/DOA4/SNR_SNS_00_4.mat'"
     ]
    }
   ],
   "source": [
    "doa_train_loader , test_dataset_list = create_dataloader(root_dataset_path, \"SNS\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f90e33-6880-48c3-9805-b088e836f829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7f7d0c-1707-4fb2-b67e-a85cd2322d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 181])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3,3), stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=181):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(1,3), stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=1)\n",
    "        self.linear = nn.Linear(25, num_classes)\n",
    "        self.adp_pool = nn.AdaptiveMaxPool2d((2, 25))\n",
    "        #self.flat = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1)\n",
    "        self.flat = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1)\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "        self.dropout1 = nn.Dropout(0.8)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.adp_pool(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.flat(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [2, 3, 5, 2])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(BasicBlock, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = torch.rand(1, 3, 10, 100)\n",
    "    model = ResNet34()\n",
    "    print(model(image).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18640939-06cb-412b-b8f6-cf08e674a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b005015-805c-48ad-bd4b-62e0cbae6d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\tprint(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deeb787-91ea-4903-8f23-202929cdb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 70\n",
    "doa = 2\n",
    "weights_dir = \"./\"\n",
    "\n",
    "autoencoder = ResNet34()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "focal_criterion = FocalLoss()\n",
    "if ('SNS_DOA_{}_300_model.pth'.format(doa) in [f for f in listdir(weights_dir) if isfile(join(weights_dir, f))]):\n",
    "    print(\"Pre-trained available for DOA_{}_300_model.pth\".format(doa))\n",
    "    autoencoder = torch.load(os.path.join(weights_dir, 'SNS_DOA_{}_300_model.pth'.format(doa)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(torch.cuda.get_device_name(0))\n",
    "\tclassification_model = autoencoder.cuda()\n",
    "\toptimizer = optim.AdamW(classification_model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\tcriterion = criterion.cuda()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012211c-0a7a-456f-9123-ed1030b769f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "loss_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "mae_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39597e07-c431-4802-9d0f-cb470ad3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"Training Starts !!!!!!!\")\n",
    "    best_valid_loss = float('Inf')\n",
    "    for i in range(num_epochs):\n",
    "        training_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        epoch_loss = 0.0\n",
    "        training_mae = 0.0\n",
    "        classification_model.train()\n",
    "        for j,(features, labels) in enumerate(doa_train_loader, 0):\n",
    "            features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            enn = classification_model(features.float())\n",
    "            auto_outputs = torch.transpose(enn, 2, 3)\n",
    "            auto_outputs = torch.reshape(auto_outputs.cuda(), (auto_outputs.shape[0], 181, doa))\n",
    "            losss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
    "            losss.backward()\n",
    "            optimizer.step()\n",
    "#           exp_scheduler.step()\n",
    "            training_loss += losss.item()\n",
    "\n",
    "            _, pred = torch.max(auto_outputs, 1)\n",
    "\n",
    "            train_total+= labels.reshape(-1).size(0)\n",
    "\n",
    "            train_correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "\n",
    "            epoch_loss += auto_outputs.shape[0] * losss.item()\n",
    "            training_mae += torch.abs(pred.reshape(-1).cuda() - labels.reshape(-1)).sum().item()  \n",
    "\n",
    "        loss_res['training'].append(training_loss/len(doa_train_loader))\n",
    "        acc_res['training'].append((100*(train_correct/train_total)))\n",
    "        mae_res['training'].append(training_mae/(64*len(doa_train_loader)*181))\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}, Training Accuracy: {:.4f}, Training MAE: {}'\n",
    "                      .format(i+1, num_epochs, training_loss/len(doa_train_loader), (100*(train_correct/train_total)), training_mae/(len(doa_train_loader)*64*181)))\n",
    "        \n",
    "        # Validation for each SNR value\n",
    "        classification_model.eval()\n",
    "        total_valdation_loss = 0\n",
    "        for val_data in range(0, len(test_dataset_list)):\n",
    "            \n",
    "            validation_loss = 0\n",
    "            validation_acc = 0\n",
    "            validation_mae = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for features, labels in test_dataset_list[val_data]:\n",
    "                    features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "                    enn = classification_model(features.float())\n",
    "                    auto_outputs = torch.transpose(enn, 2, 3)\n",
    "                    auto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, doa))\n",
    "                    loss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
    "\n",
    "                    _, pred = torch.max(auto_outputs, 1)\n",
    "                    val_total+= labels.reshape(-1).size(0)\n",
    "                    val_correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "                    validation_loss += loss.item()\n",
    "                    validation_mae += torch.abs(pred.reshape(-1).cuda() - labels.reshape(-1)).sum().item()\n",
    "\n",
    "                loss_res[10*val_data].append(validation_loss/len(test_dataset_list[val_data]))\n",
    "                acc_res[10*val_data].append((100*(val_correct/val_total)))\n",
    "                \n",
    "                mae_res[10*val_data].append(validation_mae/(128*len(test_dataset_list[val_data])*181))\n",
    "                print('SNR [{}dB], Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation MAE: {}'\n",
    "                      .format(val_data*10, validation_loss/len(test_dataset_list[val_data]), (100*(val_correct/val_total)), validation_mae/(len(test_dataset_list[val_data])*128*181)))\n",
    "\n",
    "                total_valdation_loss+=validation_loss\n",
    "            torch.save( classification_model, weights_dir+ \"/SNS_DOA_{}_300_model.pth\".format(doa))\n",
    "            if best_valid_loss > total_valdation_loss:\n",
    "                best_valid_loss = total_valdation_loss \n",
    "                # Saving Best Pre-Trained Model as .pth file\n",
    "                torch.save( classification_model, weights_dir+ \"/DOA_{}_300_best_model.pth\".format(doa))\n",
    "#         if i%10 == 0:\n",
    "#           ddf = pd.DataFrame(acc_res)\n",
    "#           ddf.to_csv(weights_dir+\"res_DOA_{}_model.csv\".format(doa))\n",
    "        print(\"\\n\")  \n",
    "\n",
    "train()\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc93d37-4a37-45fb-9a91-6724e65c5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(acc_res[0]), max(acc_res[10]), max(acc_res[20]), max(acc_res[30]), max(acc_res[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d179a0ba-900b-4990-8ec2-09a985094a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNS DOA1 200 Samples\n",
    "print(\"=== Metrics ===\")\n",
    "print(\"==== DOA-{} ====\".format(doa))\n",
    "print(\"= MAE =_= Acc =\")\n",
    "print(\"{:.4f}\".format(min(mae_res[0])), \"  {:.2f}\".format(acc_res[0][mae_res[0].index(min(mae_res[0]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[10])), \"  {:.2f}\".format(acc_res[10][mae_res[10].index(min(mae_res[10]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[20])), \"  {:.2f}\".format(acc_res[20][mae_res[20].index(min(mae_res[20]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[30])), \"  {:.2f}\".format(acc_res[30][mae_res[30].index(min(mae_res[30]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[40])), \"  {:.2f}\".format(acc_res[40][mae_res[40].index(min(mae_res[40]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57acd2-7a4e-4327-a07f-1edb0dee20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ep = range(0, len(loss_res[\"training\"]))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(ep, loss_res[\"training\"], label='Train')\n",
    "plt.plot(ep, loss_res[0], label='0 dB')\n",
    "plt.plot(ep, loss_res[10], label='10 dB')\n",
    "plt.plot(ep, loss_res[20], label='20 dB')\n",
    "plt.plot(ep, loss_res[30], label='30 dB')\n",
    "plt.plot(ep, loss_res[40], label='40 dB')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title(\"DOA - {}\".format(doa))\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e19c427-eb2a-4504-aadc-08372c6a4522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8960553845713439"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmath.phase(4-5j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8217a9fc-9dbb-45be-82d8-67a57f236b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4031242374328485"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(4-5j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b404d07f-63f1-4772-927e-4034b8171f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3], [8, 6, 7], [4, 6, 11]], [[1,2,3], [8, 6, 7], [4, 6, 11]]])\n",
    "b = torch.tensor([[[2,1,3], [8, 6, 8], [4, 6, 10]], [[2,1,3], [8, 6, 8], [4, 6, 10]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4caf7638-6c01-4036-aab4-81923df9cb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1,  1,  0],\n",
       "         [ 0,  0, -1],\n",
       "         [ 0,  0,  1]],\n",
       "\n",
       "        [[-1,  1,  0],\n",
       "         [ 0,  0, -1],\n",
       "         [ 0,  0,  1]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2b004a3-7ca5-414e-9dcb-26318891c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = torch.abs(a.reshape(-1) - b.reshape(-1)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01b9692c-8fb2-472c-80c1-70f367e4d796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0025)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(a.float() - b.float()))/181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e119a12f-4c4a-42b1-bfb1-2722fe57490d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024554941682013503"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae/(18*181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50605903-710c-429b-bc4a-caeba6c2df47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
