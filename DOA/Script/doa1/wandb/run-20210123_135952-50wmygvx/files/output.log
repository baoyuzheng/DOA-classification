GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "train.py", line 185, in <module>
    train()
  File "train.py", line 141, in train
    enn = autoencoder(features.float())
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/iiitd/Desktop/Ahmad/doa1/auto.py", line 74, in forward
    x2 = self.max_pool_2x2(x1)
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/nn/modules/pooling.py", line 159, in forward
    self.return_indices)
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/_jit_internal.py", line 247, in fn
    return if_false(*args, **kwargs)
  File "/home/iiitd/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 576, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 10.91 GiB total capacity; 982.42 MiB already allocated; 62.38 MiB free; 994.00 MiB reserved in total by PyTorch)
