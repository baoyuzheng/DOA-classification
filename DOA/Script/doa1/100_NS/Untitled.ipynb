{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65839185-9dea-4d3f-92ad-c00f99d2c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "\n",
    "#from unet import UNet\n",
    "# from auto import encoder, decoder\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "#==========================================================================\n",
    "# For Plotting loss graph\n",
    "# Bokeh\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from functools import partial\n",
    "from threading import Thread\n",
    "from tornado import gen\n",
    "# from AttRCNN_UNet import Att_R2U\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "import sys\n",
    "# from dataloader import norm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sys import getsizeof\n",
    "# import wandb\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "def create_dataloader(root_path, dataset_type, ss):\n",
    "    # Loading all dataset \n",
    "    logging.info('Data prepration started')\n",
    "    df1  = sio.loadmat(root_path + \"/SNR_{}_0_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "    df2  = sio.loadmat(root_path + \"/SNR_{}_10_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "    df3  = sio.loadmat(root_path + \"/SNR_{}_20_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "    df4  = sio.loadmat(root_path + \"/SNR_{}_30_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "    df5  = sio.loadmat(root_path + \"/SNR_{}_40_{}_{}.mat\".format(nq_type, no_doa, no_samples))\n",
    "    df = [df1, df2, df3, df4, df5]\n",
    "\n",
    "    new_train_dataset_list = []\n",
    "    new_test_dataset_list = []\n",
    "    train_dataset_list_label = []\n",
    "    test_dataset_list_label = []\n",
    "\n",
    "    dataset_len = 0\n",
    "    if dataset_type == \"SNS\":\n",
    "        dataset_len = ss//10\n",
    "    if dataset_type == \"NS\":\n",
    "        dataset_len = ss\n",
    "    logging.info('Dividing your dataset into train and test')\n",
    "    for file in df:\n",
    "        X_train, X_test, y_train, y_test = create_dataset(file, dataset_type+\"_data\")\n",
    "        new_train_dataset_list.extend(X_train)\n",
    "        new_test_dataset_list.append(X_test.tolist())\n",
    "        train_dataset_list_label.extend(y_train)\n",
    "        test_dataset_list_label.append(y_test.tolist())\n",
    "\n",
    "    # Deleteing all unused memory\n",
    "    del df\n",
    "    del df1\n",
    "    del df2\n",
    "    del df3\n",
    "    del df4\n",
    "    del df5\n",
    "\n",
    "    # Dividing into different channels\n",
    "    logging.info('Dividing your dataset into 3 channel(train)')\n",
    "\n",
    "    for idx, data in enumerate(new_train_dataset_list):\n",
    "        new = np.zeros((3, 8, dataset_len))\n",
    "        for j in range(0, data.shape[0]):\n",
    "            for k in range(0, data.shape[1]):\n",
    "                new[0][j][k] = data[j][k].real\n",
    "                new[1][j][k] = data[j][k].imag\n",
    "                new[2][j][k] = cmath.phase(data[j][k])\n",
    "        new_train_dataset_list[idx] = new\n",
    "    del new\n",
    "    logging.info('Dividing your dataset into 3 channel(test)')\n",
    "    for idx, data in enumerate(new_test_dataset_list):\n",
    "        for i, ndata in enumerate(data):\n",
    "            new = np.zeros((3, 8, dataset_len))\n",
    "            for j in range(0, 8):\n",
    "                for k in range(0, dataset_len):\n",
    "                    new[0][j][k] = ndata[j][k].real\n",
    "                    new[1][j][k] = ndata[j][k].imag\n",
    "                    new[2][j][k] = cmath.phase(ndata[j][k])\n",
    "            new_test_dataset_list[idx][i] = new\n",
    "    del new\n",
    "    # Generaring train loader\n",
    "    logging.info('Creating Train dataloader')\n",
    "    doa_train_loader = get_data(new_train_dataset_list, train_dataset_list_label, 64, True)\n",
    "    \n",
    "    # Deleteing all unused memory\n",
    "    del new_train_dataset_list\n",
    "    del train_dataset_list_label\n",
    "\n",
    "    test_dataset_list = []\n",
    "    logging.info('Creating Validation dataloader')\n",
    "    for idx, data in enumerate(new_test_dataset_list):\n",
    "        f = get_data(data, test_dataset_list_label[idx], 128, False)\n",
    "        test_dataset_list.append(f)\n",
    "\n",
    "    # Deleteing all unused memory\n",
    "    del new_test_dataset_list\n",
    "    \n",
    "    logging.info('Your dataset is ready !!')\n",
    "    return doa_train_loader ,test_dataset_list\n",
    "\n",
    "doa_train_loader , test_dataset_list = create_dataloader(root_dataset_path, nq_type, no_samples)\n",
    "\n",
    "for j,(features, labels) in enumerate(doa_train_loader, 0):\n",
    "    features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "    print(\"features\", features.size())\n",
    "    print(\"labels\", labels.size())\n",
    "    break\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3,3), stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=181):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(1,3), stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=1)\n",
    "        self.linear = nn.Linear(25, num_classes)\n",
    "        self.adp_pool = nn.AdaptiveMaxPool2d((no_doa, 25))\n",
    "        #self.flat = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1)\n",
    "        self.flat = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1)\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "        self.dropout1 = nn.Dropout(0.8)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.adp_pool(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.flat(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [2, 3, 5, 2])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(BasicBlock, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = torch.rand(1, 3, 10, 100)\n",
    "    model = ResNet34()\n",
    "    print(model(image).size())\n",
    "\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "weights_dir = root_dataset_path\n",
    "\n",
    "autoencoder = ResNet34()\n",
    "cross_criterion = nn.CrossEntropyLoss()\n",
    "focal_criterion = FocalLoss()\n",
    "# if ('{}_DOA_{}_{}_model.pth'.format(nq_type, no_doa, no_samples) in [f for f in listdir(weights_dir) if isfile(join(weights_dir, f))]):\n",
    "#     print(\"Pre-trained available for DOA_{}_{}_model.pth\".format(no_doa, no_samples))\n",
    "#     autoencoder = torch.load(os.path.join(weights_dir, '{}_DOA_{}_{}_model.pth'.format(nq_type, no_doa, no_samples)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  classification_model = autoencoder.cuda()\n",
    "  optimizer = optim.AdamW(classification_model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "  \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=0.05, patience=5, verbose=True)\n",
    "\n",
    "acc_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "loss_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "mae_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "\n",
    "mae_res\n",
    "\n",
    "%%time\n",
    "def train():\n",
    "    print(\"Training Starts !!!!!!!\")\n",
    "    best_valid_loss = float('Inf')\n",
    "    for i in range(num_epochs):\n",
    "        training_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        epoch_loss = 0.0\n",
    "        training_mae = 0.0\n",
    "        classification_model.train()\n",
    "        criterion = None\n",
    "        if (i+1)>=25 :\n",
    "          print(\"Criterion is changed to focal loss\")\n",
    "          criterion = focal_criterion.cuda()\n",
    "        else:\n",
    "          criterion = cross_criterion.cuda()\n",
    "        for j,(features, labels) in enumerate(doa_train_loader, 0):\n",
    "            features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            enn = classification_model(features.float())\n",
    "            auto_outputs = torch.transpose(enn, 2, 3)\n",
    "            auto_outputs = torch.reshape(auto_outputs.cuda(), (auto_outputs.shape[0], 181, no_doa))\n",
    "            losss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
    "            losss.backward()\n",
    "            optimizer.step()\n",
    "#           exp_scheduler.step()\n",
    "            training_loss += losss.item()\n",
    "\n",
    "            _, pred = torch.max(auto_outputs, 1)\n",
    "\n",
    "            train_total+= labels.reshape(-1).size(0)\n",
    "\n",
    "            train_correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "\n",
    "            epoch_loss += auto_outputs.shape[0] * losss.item()\n",
    "            training_mae += torch.mean(torch.abs(pred.float().cuda() - labels.float()))\n",
    "\n",
    "        loss_res['training'].append(training_loss/len(doa_train_loader))\n",
    "        acc_res['training'].append((100*(train_correct/train_total)))\n",
    "        mae_res['training'].append(training_mae/(len(doa_train_loader)*181))\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}, Training Accuracy: {:.4f}, Training MAE: {}'\n",
    "                      .format(i+1, num_epochs, training_loss/len(doa_train_loader), (100*(train_correct/train_total)), training_mae/(len(doa_train_loader)*181)))\n",
    "        \n",
    "        # Validation for each SNR value\n",
    "        classification_model.eval()\n",
    "        total_valdation_loss = 0\n",
    "        zero_val = 0\n",
    "        for val_data in range(0, len(test_dataset_list)):\n",
    "            \n",
    "            validation_loss = 0\n",
    "            validation_acc = 0\n",
    "            validation_mae = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for features, labels in test_dataset_list[val_data]:\n",
    "                    features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "                    enn = classification_model(features.float())\n",
    "                    auto_outputs = torch.transpose(enn, 2, 3)\n",
    "                    auto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, no_doa))\n",
    "                    loss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
    "\n",
    "                    _, pred = torch.max(auto_outputs, 1)\n",
    "                    val_total+= labels.reshape(-1).size(0)\n",
    "                    val_correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "                    validation_loss += loss.item()\n",
    "                    validation_mae += torch.mean(torch.abs(pred.float().cuda() - labels.float()))\n",
    "\n",
    "                loss_res[10*val_data].append(validation_loss/len(test_dataset_list[val_data]))\n",
    "                acc_res[10*val_data].append((100*(val_correct/val_total)))\n",
    "                \n",
    "                mae_res[10*val_data].append(validation_mae/(len(test_dataset_list[val_data])*181))\n",
    "\n",
    "                print('SNR [{}dB], Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation MAE: {}'\n",
    "                      .format(val_data*10, validation_loss/len(test_dataset_list[val_data]), (100*(val_correct/val_total)), validation_mae/(len(test_dataset_list[val_data])*181)))\n",
    "\n",
    "                total_valdation_loss+=validation_loss\n",
    "\n",
    "        scheduler.step(loss_res[0][len(loss_res[0])-1])\n",
    "        # print(\"Odb Loss => \", loss_res[0][len(loss_res[0])-1])\n",
    "        # print(\"Lr -> \", optimizer.state_dict()[\"param_groups\"][0]['lr'])\n",
    "        torch.save( classification_model, weights_dir+ \"/{}_DOA_{}_{}_model.pth\".format(nq_type, no_doa, no_samples))\n",
    "        if best_valid_loss > total_valdation_loss:\n",
    "            best_valid_loss = total_valdation_loss \n",
    "            # Saving Best Pre-Trained Model as .pth file\n",
    "            torch.save( classification_model, weights_dir+ \"/{}_DOA_{}_{}_best_model.pth\".format(nq_type, no_doa, no_samples))\n",
    "#         if i%10 == 0:\n",
    "#           ddf = pd.DataFrame(acc_res)\n",
    "#           ddf.to_csv(weights_dir+\"res_DOA_{}_model.csv\".format(doa))\n",
    "        print(\"\\n\")\n",
    "\n",
    "train()\n",
    "print(\"Training Complete\")\n",
    "\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "##DOA6\n",
    "\n",
    "# SNS DOA-2 200 Samples\n",
    "print(\"=== Metrics ===\")\n",
    "print(\"==== DOA-{} ====\".format(doa))\n",
    "print(\"= MAE =_= Acc =\")\n",
    "print(\"{:.4f}\".format(min(mae_res[0])), \"  {:.2f}\".format(acc_res[0][mae_res[0].index(min(mae_res[0]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[10])), \"  {:.2f}\".format(acc_res[10][mae_res[10].index(min(mae_res[10]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[20])), \"  {:.2f}\".format(acc_res[20][mae_res[20].index(min(mae_res[20]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[30])), \"  {:.2f}\".format(acc_res[30][mae_res[30].index(min(mae_res[30]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[40])), \"  {:.2f}\".format(acc_res[40][mae_res[40].index(min(mae_res[40]))]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ep = range(0, len(loss_res[\"training\"]))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(ep, loss_res[\"training\"], label='Train')\n",
    "plt.plot(ep, loss_res[0], label='0 dB')\n",
    "plt.plot(ep, loss_res[10], label='10 dB')\n",
    "plt.plot(ep, loss_res[20], label='20 dB')\n",
    "plt.plot(ep, loss_res[30], label='30 dB')\n",
    "plt.plot(ep, loss_res[40], label='40 dB')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title(\"DOA - {}\".format(doa))\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ep = range(0, len(loss_res[\"training\"]))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(ep, acc_res[\"training\"], label='Train')\n",
    "plt.plot(ep, acc_res[0], label='0 dB')\n",
    "plt.plot(ep, acc_res[10], label='10 dB')\n",
    "plt.plot(ep, acc_res[20], label='20 dB')\n",
    "plt.plot(ep, acc_res[30], label='30 dB')\n",
    "plt.plot(ep, acc_res[40], label='40 dB')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.savefig(\"DOA3_resnet34\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##DOA 5\n",
    "\n",
    "# SNS DOA-2 200 Samples\n",
    "print(\"=== Metrics ===\")\n",
    "print(\"==== DOA-{} ====\".format(no_doa))\n",
    "print(\"= MAE =_= Acc =\")\n",
    "print(\"{:.4f}\".format(min(mae_res[0])), \"  {:.2f}\".format(acc_res[0][mae_res[0].index(min(mae_res[0]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[10])), \"  {:.2f}\".format(acc_res[10][mae_res[10].index(min(mae_res[10]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[20])), \"  {:.2f}\".format(acc_res[20][mae_res[20].index(min(mae_res[20]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[30])), \"  {:.2f}\".format(acc_res[30][mae_res[30].index(min(mae_res[30]))]))\n",
    "print(\"{:.4f}\".format(min(mae_res[40])), \"  {:.2f}\".format(acc_res[40][mae_res[40].index(min(mae_res[40]))]))\n",
    "\n",
    "\n",
    "\n",
    "# SNS DOA-2 200 Samples\n",
    "print(\"=== Metrics ===\")\n",
    "print(\"==== DOA-{} ====\".format(no_doa))\n",
    "print(\"= MAE =_= Acc =\")\n",
    "print(\"{:.4f}\".format(max(acc_res[0])), \"  {:.4f}\".format(mae_res[0][acc_res[0].index(max(acc_res[0]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[10])), \"  {:.4f}\".format(mae_res[10][acc_res[10].index(max(acc_res[10]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[20])), \"  {:.4f}\".format(mae_res[20][acc_res[20].index(max(acc_res[20]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[30])), \"  {:.4f}\".format(mae_res[30][acc_res[30].index(max(acc_res[30]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[40])), \"  {:.4f}\".format(mae_res[40][acc_res[40].index(max(acc_res[40]))]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ep = range(0, len(loss_res[\"training\"]))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(ep, loss_res[\"training\"], label='Train')\n",
    "plt.plot(ep, loss_res[0], label='0 dB')\n",
    "plt.plot(ep, loss_res[10], label='10 dB')\n",
    "plt.plot(ep, loss_res[20], label='20 dB')\n",
    "plt.plot(ep, loss_res[30], label='30 dB')\n",
    "plt.plot(ep, loss_res[40], label='40 dB')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title(\"DOA - {}\".format(no_doa))\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ep = range(0, len(loss_res[\"training\"]))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(ep, loss_res[\"training\"], label='Train')\n",
    "plt.plot(ep, loss_res[0], label='0 dB')\n",
    "plt.plot(ep, loss_res[10], label='10 dB')\n",
    "plt.plot(ep, loss_res[20], label='20 dB')\n",
    "plt.plot(ep, loss_res[30], label='30 dB')\n",
    "plt.plot(ep, loss_res[40], label='40 dB')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title(\"DOA - {}\".format(no_doa))\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## DOA 5 (Unet + resnet)\n",
    "\n",
    "# SNS DOA-2 200 Samples\n",
    "print(\"=== Metrics ===\")\n",
    "print(\"==== DOA-{} ====\".format(no_doa))\n",
    "print(\"= MAE =_= Acc =\")\n",
    "print(\"{:.4f}\".format(max(acc_res[0])), \"  {:.4f}\".format(mae_res[0][acc_res[0].index(max(acc_res[0]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[10])), \"  {:.4f}\".format(mae_res[10][acc_res[10].index(max(acc_res[10]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[20])), \"  {:.4f}\".format(mae_res[20][acc_res[20].index(max(acc_res[20]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[30])), \"  {:.4f}\".format(mae_res[30][acc_res[30].index(max(acc_res[30]))]))\n",
    "print(\"{:.4f}\".format(max(acc_res[40])), \"  {:.4f}\".format(mae_res[40][acc_res[40].index(max(acc_res[40]))]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ep = range(0, len(loss_res[\"training\"]))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(ep, loss_res[\"training\"], label='Train')\n",
    "plt.plot(ep, loss_res[0], label='0 dB')\n",
    "plt.plot(ep, loss_res[10], label='10 dB')\n",
    "plt.plot(ep, loss_res[20], label='20 dB')\n",
    "plt.plot(ep, loss_res[30], label='30 dB')\n",
    "plt.plot(ep, loss_res[40], label='40 dB')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title(\"DOA - {}\".format(no_doa))\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b21ca97-100a-4c11-b586-6a6e74d0ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "\n",
    "#from unet import UNet\n",
    "# from auto import encoder, decoder\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "#==========================================================================\n",
    "# For Plotting loss graph\n",
    "# Bokeh\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from functools import partial\n",
    "from threading import Thread\n",
    "from tornado import gen\n",
    "# from AttRCNN_UNet import Att_R2U\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "import sys\n",
    "# from dataloader import norm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sys import getsizeof\n",
    "# import wandb\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e6de8d-9f28-44dc-a9ca-2d18e192eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nq_type = \"NS\" # Sub-Nyquist or Nyquist sample\n",
    "no_samples = 100\n",
    "no_doa = 4\n",
    "\n",
    "root_dataset_path =  (\"/content/drive/MyDrive/DOA/{}/{}/DOA{}\".format(nq_type, no_samples, no_doa))\n",
    "\n",
    "def create_dataset(df, dataset_type):\n",
    "  \n",
    "    data = np.transpose(df[dataset_type], (2, 0, 1))\n",
    "    label = df['DOA']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.15, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_data(train, test, batch_size, train_bool=True):\n",
    "    class DOA_dataset(Dataset):\n",
    "        def __init__(self, train, test):\n",
    "            self.x = torch.from_numpy(np.array(train))\n",
    "            self.y = torch.from_numpy(np.asarray(test))\n",
    "            self.n_sample = len(self.y)\n",
    "        def __getitem__(self, index):\n",
    "            return self.x[index], self.y[index]\n",
    "        def __len__(self):\n",
    "            return self.n_sample\n",
    "\n",
    "\n",
    "    dataset = DOA_dataset(train, test)\n",
    "    \n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_bool)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf130f23-83c7-4cc7-a614-75f53369d098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
