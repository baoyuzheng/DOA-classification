{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preTrain.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPz/8U9Kt+tq/TVdK2fToKr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j7NiQAAhgQRu"},"source":["# UNeT Arch"]},{"cell_type":"code","metadata":{"id":"kkI2eHJUgG4K"},"source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import logging\n","from torchvision import models\n","\n","def double_conv(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(3,2),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(3,2), padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_conv1(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(3,2),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(3,2),stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_conv2(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(3,3),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(3,3), padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_conv3(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=3,padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=3, padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def double_conv4(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.Conv2d(in_c, out_c, kernel_size=(2, 3),padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_c, out_c, kernel_size=(2, 3), padding=1 ,stride=1, bias=True),\n","        nn.BatchNorm2d(out_c),\n","        nn.ReLU(inplace=True))\n","    return conv\n","\n","def up_conv(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2))\n","    return conv\n","\n","def up_conv1(in_c, out_c):\n","    conv = nn.Sequential(\n","        nn.ConvTranspose2d(in_c, out_c, kernel_size=(3,2),stride=2))\n","    return conv\n","\n","def crop_ig(one, two):\n","    ts = two.size()[3]\n","    tens = one.size()[3]\n","    print(one.size()[3], two.size()[3])\n","    delta = abs(tens - ts)\n","    delta = delta\n","    return one[:, :, delta: tens-delta, delta: tens-delta]\n","\n","class UNet(nn.Module):\n","    def __init__(self,img_ch=2,output_ch=2):\n","        super(UNet, self).__init__()\n","\n","        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=(2,1), stride=2)\n","        self.down_conv_1 = double_conv1(img_ch, 64)\n","        self.down_conv_2 = double_conv2(64, 128)\n","        self.down_conv_3 = double_conv3(128, 256)\n","\n","        self.up_trans_3 = up_conv1(256, 128)\n","        self.up_conv_3 = double_conv2(256, 128)\n","        \n","        self.up_trans_4 = up_conv(128, 64)\n","        self.up_conv_4 = double_conv4(128, 64)\n","        \n","        self.out = nn.Conv2d(\n","            in_channels=64,\n","            out_channels=output_ch,\n","            kernel_size=1,stride=1,padding=0)\n","    \n","    def forward(self, image):\n","        # encoder\n","        x1 = self.down_conv_1(image)\n","        x2 = self.max_pool_2x2(x1)\n","        x3 = self.down_conv_2(x2)\n","        x4 = self.max_pool_2x2(x3)\n","        x5 = self.down_conv_3(x4)\n","        \n","        # decoder\n","        x = self.up_trans_3(x5)\n","        x = self.up_conv_3(torch.cat([x, x3], 1))\n","        x = self.up_trans_4(x)\n","        x = self.up_conv_4(torch.cat([x, x1], 1))\n","        # output\n","        x = self.out(x)\n","    \n","        return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PqLZJwEQgg_u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607709252807,"user_tz":-330,"elapsed":44422,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}},"outputId":"8b375e66-faa9-4021-e720-c1c717f75919"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pBeG7cJxgrsE"},"source":["# torch.manual_seed(17)\r\n","\r\n","# if torch.cuda.is_available():\r\n","#     device = torch.device('cuda:0')\r\n","#     torch.backends.cudnn.deterministic = True\r\n","#     torch.backends.cudnn.benchmark = False\r\n","# else:\r\n","#     device = torch.device('cpu')\r\n","\r\n","# print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSVrBKJvgr3f"},"source":["import scipy.io as sio\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np \n","import math\n","import pandas as pd\n","import cmath\n","from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sveGsJS5twsR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jb--8S7ap4ga"},"source":[" from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrJTgMeFt-Rs"},"source":["\n","df1 = sio.loadmat(\"./gdrive/MyDrive/DOA/preTrain/SNR_NS_ALL_50000_1.mat\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-o2izQHVtBOG"},"source":["\n","max_r = 0.2655554840881041\n","max_i = 0.247665118111416\n","max_p = 3.141592030578154\n","min_r = -0.24698739374092077\n","min_i = -0.23956457294409056\n","min_p = -3.1415875527493355\n","\n","class DOA_dataset(Dataset):\n","    def __init__(self, df):\n","        transp = np.transpose(df['NS_data'], (2, 0, 1))\n","        new = np.zeros((300000, 3, 8, 100))\n","        for i in range(0, transp.shape[0]):\n","            for j in range(0, transp.shape[1]):\n","                for k in range(0, transp.shape[2]):\n","                    new[i][0][j][k] = (transp[i][j][k].real - min_r)/(max_r-min_r)\n","                    new[i][1][j][k] = (transp[i][j][k].imag - min_i)/(max_i-min_i)\n","                    new[i][2][j][k] = (cmath.phase(transp[i][j][k]) - min_p)/(max_p-min_p)\n","\n","        self.x = torch.from_numpy(new)\n","        self.y = torch.from_numpy(np.asarray(df['DOA']))\n","        self.n_sample = len(self.y)\n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index]\n","    def __len__(self):\n","        return self.n_sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m917X2hPhCEA"},"source":["dataset = DOA_dataset(df1)\n","\n","validation_split = .3\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","\n","#dataloader = DataLoader(dataset=dff, batch_size=100, shuffle=True,  num_workers=2)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n","                                           sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=256,\n","                                                sampler=valid_sampler)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5n7lqEXghCHy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607677895219,"user_tz":-330,"elapsed":881,"user":{"displayName":"Mohammad Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1sqT9k-ZJhDQXM7x7dup6__HgCR7MEKRseC9t6w=s64","userId":"07657306696827110934"}},"outputId":"34429728-d098-41b1-a009-5417f79a327a"},"source":["unet = UNet()\n","\n","optimizer = optim.Adam(unet.parameters(), lr=0.001)\n","\n","criterion = nn.BCELoss ()\n","\n","# Using GPU if available \n","if torch.cuda.is_available():\n","  print(torch.cuda.get_device_name(0))\n","  unet = unet.cuda()\n","  criterion = criterion.cuda()\n","\n","# exp_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma = 0.1)\n","def train():\n","  ''' Funtion for training  '''\n","\n","  for i in range(200):\n","    # Dataset Training Loss\n","    training_loss = 0\n","    for features, labels in train_loader:\n","      inputs, labels = Variable(features.cuda()), Variable(labels.cuda())\n","      optimizer.zero_grad()\n","      output = unet(inputs.float().cuda())\n","      output = torch.sigmoid(output)\n","      losss = criterion(output, inputs.float().cuda())\n","      losss.backward()\n","      optimizer.step()\n","      \n","      # exp_scheduler.step()\n","      training_loss += losss.item()\n","\n","    # Dataset Validation\n","    validation_loss = 0\n","    with torch.no_grad():\n","      for features, labels in validation_loader:\n","        inputs, labels = Variable(features.cuda()), Variable(labels.cuda())\n","        output = unet(inputs.float().cuda())\n","        output = torch.sigmoid(output)\n","        loss = criterion(output, inputs.float().cuda())\n","        validation_loss += loss.item()\n","    print(\"Epoch {} - Traningloss: {}\".format(i+1, training_loss/len(train_loader)))\n","    print(\"Validationloss: {}\".format( validation_loss/len(validation_loader)))\n","\n","  # Saving Model as .pth file\n","  torch.save(unet.state_dict(), \"./Attention_block_wieghts_phase.pth\")\n","  print(\"Training complete, model weights are saved\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KEu8eO9dhCK4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a69215b1-0469-47b2-9aa6-3b9300ee6e14"},"source":["train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 - Traningloss: 0.3005560276183215\n","Validationloss: 0.14635395569105944\n","Epoch 2 - Traningloss: 0.11316394778815182\n","Validationloss: 0.08797298185527325\n","Epoch 3 - Traningloss: 0.08479482010006904\n","Validationloss: 0.0744174225255847\n","Epoch 4 - Traningloss: 0.07720596268773079\n","Validationloss: 0.06982004782184958\n","Epoch 5 - Traningloss: 0.07456880936568433\n","Validationloss: 0.06806276862819989\n","Epoch 6 - Traningloss: 0.07302839715372432\n","Validationloss: 0.06709852674975991\n","Epoch 7 - Traningloss: 0.07226866846057502\n","Validationloss: 0.06592612372090419\n","Epoch 8 - Traningloss: 0.07180313833735207\n","Validationloss: 0.0657788875202338\n","Epoch 9 - Traningloss: 0.0712685451419516\n","Validationloss: 0.06553743702049057\n","Epoch 10 - Traningloss: 0.07115896230732853\n","Validationloss: 0.06587582019468148\n","Epoch 11 - Traningloss: 0.07119608938016675\n","Validationloss: 0.06556447595357895\n","Epoch 12 - Traningloss: 0.07089409855279055\n","Validationloss: 0.0877057119893531\n","Epoch 13 - Traningloss: 0.0721733203327114\n","Validationloss: 0.06619893076519172\n","Epoch 14 - Traningloss: 0.07138931920582597\n","Validationloss: 0.065753609718134\n","Epoch 15 - Traningloss: 0.07095295268703591\n","Validationloss: 0.06448353920131922\n","Epoch 16 - Traningloss: 0.07062669972127134\n","Validationloss: 0.06482004451875885\n","Epoch 17 - Traningloss: 0.07089270847764882\n","Validationloss: 0.06546137916545074\n","Epoch 18 - Traningloss: 0.07084580797024749\n","Validationloss: 0.06472443028663595\n","Epoch 19 - Traningloss: 0.07063576423309066\n","Validationloss: 0.06496868763739864\n","Epoch 20 - Traningloss: 0.07060845548456365\n","Validationloss: 0.06537050474435091\n","Epoch 21 - Traningloss: 0.07067578539929607\n","Validationloss: 0.06464486538122098\n","Epoch 22 - Traningloss: 0.07049969084222209\n","Validationloss: 0.064479256204019\n","Epoch 23 - Traningloss: 0.07072847397489981\n","Validationloss: 0.06378516554832458\n","Epoch 24 - Traningloss: 0.07047487944364547\n","Validationloss: 0.06459572647387783\n","Epoch 25 - Traningloss: 0.07079374305903911\n","Validationloss: 0.06459117556611697\n","Epoch 26 - Traningloss: 0.0706963574513793\n","Validationloss: 0.06401861016638577\n","Epoch 27 - Traningloss: 0.07061181484975598\n","Validationloss: 0.064292227383703\n","Epoch 28 - Traningloss: 0.07035518562929197\n","Validationloss: 0.06514193319405119\n","Epoch 29 - Traningloss: 0.07027347431602803\n","Validationloss: 0.06500267478016515\n","Epoch 30 - Traningloss: 0.07051139270717448\n","Validationloss: 0.0651315061065058\n","Epoch 31 - Traningloss: 0.07058546563441104\n","Validationloss: 0.06556763872504234\n","Epoch 32 - Traningloss: 0.07067081372846257\n","Validationloss: 0.0644065486267209\n","Epoch 33 - Traningloss: 0.0702539656480605\n","Validationloss: 0.06397691850240032\n","Epoch 34 - Traningloss: 0.07041636676612226\n","Validationloss: 0.06455533726451297\n","Epoch 35 - Traningloss: 0.07041325242343274\n","Validationloss: 0.06511774752289057\n","Epoch 36 - Traningloss: 0.07049058147452095\n","Validationloss: 0.06482963729649782\n","Epoch 37 - Traningloss: 0.07073192887685516\n","Validationloss: 0.06482644169591367\n","Epoch 38 - Traningloss: 0.07032891872592947\n","Validationloss: 0.0643943645991385\n","Epoch 39 - Traningloss: 0.0706520797346126\n","Validationloss: 0.0652521272810797\n","Epoch 40 - Traningloss: 0.07041030058806592\n","Validationloss: 0.06453962080801527\n","Epoch 41 - Traningloss: 0.07089123632758856\n","Validationloss: 0.06468508889277776\n","Epoch 42 - Traningloss: 0.07048685423691164\n","Validationloss: 0.0645306281124552\n","Epoch 43 - Traningloss: 0.07056073126467792\n","Validationloss: 0.06582429120317101\n","Epoch 44 - Traningloss: 0.07031600200994448\n","Validationloss: 0.06524752080440521\n","Epoch 45 - Traningloss: 0.07023205254226923\n","Validationloss: 0.06510397349484265\n","Epoch 46 - Traningloss: 0.07031424656849015\n","Validationloss: 0.06438261984537046\n","Epoch 47 - Traningloss: 0.07031314911490137\n","Validationloss: 0.06466967472806573\n","Epoch 48 - Traningloss: 0.07039567939937115\n","Validationloss: 0.06452480152559777\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UWznxtfgnsAI"},"source":["  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_ivSjmo8b3y"},"source":["import matplotlib.pyplot as plt\r\n","plt.figure(figsize=(10, 8))\r\n","plt.plot(epc_list, train_loss_list, label='Train')\r\n","plt.plot(epc_list, val_loss_list, label='Valid')\r\n","plt.xlabel('Epochs', fontsize=14)\r\n","plt.ylabel('Loss', fontsize=14)\r\n","plt.legend(fontsize=14)\r\n","plt.show()"],"execution_count":null,"outputs":[]}]}