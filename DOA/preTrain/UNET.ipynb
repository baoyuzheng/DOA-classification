{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sealed-algeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "def single_conv(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return conv\n",
    "\n",
    "def double_conv1(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=(3,2),padding=1 ,stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=(3,2),stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return conv\n",
    "\n",
    "def double_conv2(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=(3,3),padding=1 ,stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=(3,3), padding=1 ,stride=1, bias=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return conv\n",
    "\n",
    "\n",
    "def up_conv1(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_c, out_c, kernel_size=(2, 3), stride=2))\n",
    "    return conv\n",
    "    \n",
    "def up_conv2(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_c, out_c, kernel_size=(2,2), stride=2))\n",
    "    return conv\n",
    "\n",
    "\n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "\n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RRCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x+x1\n",
    "\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n",
    "\n",
    "\n",
    "class Att_R2U(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=3,t=2):\n",
    "        super(Att_R2U, self).__init__()\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.RCNN1 = RRCNN_block(img_ch, 64, t=t)\n",
    "        self.RCNN2 = RRCNN_block(64, 128, t=t)\n",
    "        self.RCNN3 = RRCNN_block(128, 256, t=t)\n",
    "\n",
    "        self.up_trans_1 = up_conv1(256, 128)\n",
    "        self.Att1 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_RRCNN1 = RRCNN_block(256, 128,t=t)\n",
    "        \n",
    "        self.up_trans_2 = up_conv2(128, 64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_RRCNN2 = RRCNN_block(128, 64,t=t)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=output_ch,\n",
    "            kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # encoder\n",
    "        # print(\"Input Image            => \", image.size())\n",
    "        # print(\"Encoder =================\")\n",
    "        x1 = self.RCNN1(image)\n",
    "        # print(\"Conv3x2, S1, P1        => \", x1.size())\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        # print(\"max_pool_2x1           => \", x2.size())\n",
    "        x3 = self.RCNN2(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "        # print(\"Conv3x3, S1, P1        => \", x3.size())\n",
    "        x4 = self.max_pool_2x2(x3)\n",
    "        # print(\"max_pool_2x1           => \", x4.size())\n",
    "        x5 = self.RCNN3(x4)\n",
    "        x5 = self.dropout(x5)\n",
    "        # print(\"Conv3x3, S1, P1        => \", x5.size())\n",
    "        \n",
    "        \n",
    "        # decoder\n",
    "        # print(\"Decoder =================\")\n",
    "        x = self.up_trans_1(x5)\n",
    "        # print(\"up_trans_1x18, S3, P0  => \", x.size())\n",
    "        x3 = nn.functional.interpolate(x3, (x.size()[2], x.size()[3]))\n",
    "        x3 = self.Att1(g=x,x=x3)\n",
    "        x = self.Up_RRCNN1(torch.cat([x, x3], 1))\n",
    "        x = self.dropout(x)\n",
    "        # print(\"up_conv_3x3, S1, P1    => \", x.size())\n",
    "\n",
    "        x = self.up_trans_2(x)\n",
    "        # print(\"up_trans_2x2, S2, P0   => \", x.size())\n",
    "        x1 = nn.functional.interpolate(x1, (x.size()[2], x.size()[3]))\n",
    "        x1 = self.Att2(g=x,x=x1)\n",
    "        x = self.Up_RRCNN2(torch.cat([x, x1], 1))\n",
    "        x = self.dropout(x)\n",
    "        # print(\"up_conv_2x3, s1, p1    => \", x.size())\n",
    "        # output\n",
    "        x = self.out(x)\n",
    "#         print(x.size())\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     print(\"start\")\n",
    "    image = torch.rand(1, 3, 8, 10)\n",
    "    model = Att_R2U()\n",
    "    print(model(image).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "breeding-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "import cmath\n",
    "\n",
    "#from unet import UNet\n",
    "# from auto import encoder, decoder\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "#==========================================================================\n",
    "# For Plotting loss graph\n",
    "# Bokeh\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from functools import partial\n",
    "from threading import Thread\n",
    "from tornado import gen\n",
    "# from AttRCNN_UNet import Att_R2U\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "import sys\n",
    "# from dataloader import norm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sys import getsizeof\n",
    "# import wandb\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "common-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_list = []\n",
    "test_dataset_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proved-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_00_3.mat\")\n",
    "df2  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_10_3.mat\")\n",
    "df3  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_20_3.mat\")\n",
    "df4  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_30_3.mat\")\n",
    "df5  = sio.loadmat(\"../SNS_dataset/DOA3/SNR_SNS_40_3.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c01e51-e500-4985-b122-d8227b88fc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10, 100000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['SNS_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220d4cf-95e3-4cf3-a765-1be696986d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5795b503-7b4d-495c-bb71-2f9237f64a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(df):\n",
    "    # print(df[\"DOA\"].shape)\n",
    "    # print(df['NS_data'].shape)\n",
    "    transp = np.transpose(df['SNS_data'], (2, 0, 1))\n",
    "    new = np.zeros((100000, 3, 8, 10))\n",
    "    for i in range(0, transp.shape[0]):\n",
    "        for j in range(0, transp.shape[1]):\n",
    "            for k in range(0, transp.shape[2]):\n",
    "                new[i][0][j][k] = transp[i][j][k].real\n",
    "                new[i][1][j][k] = transp[i][j][k].imag\n",
    "                new[i][2][j][k] = cmath.phase(transp[i][j][k])\n",
    "\n",
    "    max_r =  -10000000000000\n",
    "    min_r = 10000000000000\n",
    "    max_i =  -10000000000000\n",
    "    min_i = 10000000000000\n",
    "    max_p =  -10000000000000\n",
    "    min_p = 10000000000000\n",
    "\n",
    "    for i in range(0, new.shape[0]):\n",
    "        for j in range(0, new.shape[1]):\n",
    "            for k in range(0, new.shape[2]):\n",
    "                if new[i][0][j][k] > max_r :\n",
    "                    max_r = new[i][0][j][k]\n",
    "                if new[i][0][j][k] < min_r:\n",
    "                    min_r = new[i][0][j][k]\n",
    "                if new[i][1][j][k] > max_i :\n",
    "                    max_i = new[i][1][j][k]\n",
    "                if new[i][1][j][k] < min_i:\n",
    "                    min_i = new[i][1][j][k]\n",
    "                if new[i][2][j][k] > max_p :\n",
    "                    max_p = new[i][2][j][k]\n",
    "                if new[i][2][j][k] < min_p:\n",
    "                    min_p = new[i][2][j][k]\n",
    "    print(\"St\")\n",
    "    print(max_r, max_i, max_p, min_r, min_i, min_p)\n",
    "\n",
    "    ll = []\n",
    "    ll.append(max_r)\n",
    "    ll.append(max_i)\n",
    "    ll.append(max_p)\n",
    "    ll.append(min_r)\n",
    "    ll.append(min_i)\n",
    "    ll.append(min_p)\n",
    "    return ll\n",
    "\n",
    "# import glob\n",
    "\n",
    "# files = glob.glob(\"../SNS_dataset/DOA3/*.mat\")\n",
    "# for i in files:\n",
    "#     norm(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "previous-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [df1, df2, df3, df4, df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e837e42-9883-4f7d-ad93-8bffb0bd0005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5acd579-981a-44da-8d73-ff98b1b0cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St\n",
      "1.7603497768257328 1.8036064114959378 3.1415914753655074 -1.5408234206667115 -1.64243127488387 -3.141592026878224\n",
      "St\n",
      "1.1873229159813352 1.2741685515426118 3.1415907781264107 -1.2686789942768135 -1.0959808798207746 -3.1415913390329036\n",
      "St\n",
      "1.2494845116189814 1.1475172132657645 3.1415883679758365 -1.103939129321764 -1.476612154630019 -3.141592176120324\n",
      "St\n",
      "1.2186437738672047 1.15386135785642 3.1415920649708426 -1.1887943888211567 -1.1883723318375439 -3.1415911808443\n",
      "St\n",
      "1.2827083289095111 1.22990870811363 3.1415921643510143 -1.231530100650953 -1.0521069833637557 -3.1415870280529434\n"
     ]
    }
   ],
   "source": [
    "norm_list = [norm(i) for i in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "driving-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10, 100000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df1[\"SNS_data\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cross-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset_list = []\n",
    "new_test_dataset_list = []\n",
    "train_dataset_list_label = []\n",
    "test_dataset_list_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    data = np.transpose(df['SNS_data'], (2, 0, 1))\n",
    "    label = df['DOA']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.05, random_state=42)\n",
    "    new_train_dataset_list.extend(X_train)\n",
    "    new_test_dataset_list.append(X_test.tolist())\n",
    "    train_dataset_list_label.extend(y_train)\n",
    "    test_dataset_list_label.append(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unexpected-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in df:\n",
    "    create_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "qualified-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del df4\n",
    "del df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fabulous-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3472d60-09d2-4c1c-8d53-7c87087ef56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7603497768257328"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(norm_list[0][0], norm_list[1][0], norm_list[2][0], norm_list[3][0], norm_list[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db0a605-0ab4-47e4-9e74-bc12920e7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(new_train_dataset_list):\n",
    "    max_r = max(norm_list[0][0], norm_list[1][0], norm_list[2][0], norm_list[3][0], norm_list[4][0])\n",
    "    max_i = max(norm_list[0][1], norm_list[1][1], norm_list[2][1], norm_list[3][1], norm_list[4][1])\n",
    "    max_p = max(norm_list[0][2], norm_list[1][2], norm_list[2][2], norm_list[3][2], norm_list[4][2])\n",
    "    min_r = min(norm_list[0][3], norm_list[1][3], norm_list[2][3], norm_list[3][3], norm_list[4][3])\n",
    "    min_i = min(norm_list[0][4], norm_list[1][4], norm_list[2][4], norm_list[3][4], norm_list[4][4])\n",
    "    min_p = min(norm_list[0][5], norm_list[1][5], norm_list[2][5], norm_list[3][5], norm_list[4][5])\n",
    "#     print(type((data[0][0])))\n",
    "    new = np.zeros((3, 8, 10))\n",
    "#     print(type(new[0][0][0]))\n",
    "    for j in range(0, data.shape[0]):\n",
    "        for k in range(0, data.shape[1]):\n",
    "            new[0][j][k] =  (data[j][k].real  - min_r)/(max_r-min_r)\n",
    "            new[1][j][k] =  (data[j][k].imag - min_i)/(max_i-min_i)\n",
    "            new[2][j][k] = (cmath.phase(data[j][k]) - min_p)/ (max_p - min_p)\n",
    "    new_train_dataset_list[idx] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47db8d1e-362e-469a-b9da-3827588119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(new_test_dataset_list):\n",
    "    for i, ndata in enumerate(data):\n",
    "        max_r = max(norm_list[0][0], norm_list[1][0], norm_list[2][0], norm_list[3][0], norm_list[4][0])\n",
    "        max_i = max(norm_list[0][1], norm_list[1][1], norm_list[2][1], norm_list[3][1], norm_list[4][1])\n",
    "        max_p = max(norm_list[0][2], norm_list[1][2], norm_list[2][2], norm_list[3][2], norm_list[4][2])\n",
    "        min_r = min(norm_list[0][3], norm_list[1][3], norm_list[2][3], norm_list[3][3], norm_list[4][3])\n",
    "        min_i = min(norm_list[0][4], norm_list[1][4], norm_list[2][4], norm_list[3][4], norm_list[4][4])\n",
    "        min_p = min(norm_list[0][5], norm_list[1][5], norm_list[2][5], norm_list[3][5], norm_list[4][5])\n",
    "        new = np.zeros((3, 8, 10))\n",
    "        for j in range(0, 8):\n",
    "            for k in range(0, 10):\n",
    "                new[0][j][k] =  (ndata[j][k].real  - min_r)/(max_r-min_r)\n",
    "                new[1][j][k] =  (ndata[j][k].imag - min_i)/(max_i-min_i)\n",
    "                new[2][j][k] = (cmath.phase(ndata[j][k]) - min_p)/ (max_p - min_p)\n",
    "        new_test_dataset_list[idx][i] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a962c9cc-b765-40f8-8c68-fc1b8d27178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, test, batch_size, train_bool=True):\n",
    "    class DOA_dataset(Dataset):\n",
    "        def __init__(self, train, test):\n",
    "            self.x = torch.from_numpy(np.array(train))\n",
    "            self.y = torch.from_numpy(np.asarray(test))\n",
    "            self.n_sample = len(self.y)\n",
    "        def __getitem__(self, index):\n",
    "            return self.x[index], self.y[index]\n",
    "        def __len__(self):\n",
    "            return self.n_sample\n",
    "\n",
    "\n",
    "    dataset = DOA_dataset(train, test)\n",
    "    \n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_bool)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d8f6b3-5ed0-4fb0-a690-3b63a0b463db",
   "metadata": {},
   "outputs": [],
   "source": [
    "doa2_train_loader = get_data(new_train_dataset_list, train_dataset_list_label, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23363836-0597-40a6-9e2d-490bf17a8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_train_dataset_list\n",
    "del train_dataset_list_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "340bcf9d-3c26-45e8-8ddf-eccf33fb19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57058d96-7dbb-4945-a778-5cb4b50e41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(new_test_dataset_list):\n",
    "    print(type(data))\n",
    "    f = get_data(data, test_dataset_list_label[idx], 128, False)\n",
    "    test_dataset_list.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb01ccf-99f5-4c5f-a431-0ae4ddc4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_test_dataset_list\n",
    "del new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ecf8845-dc32-4ac2-9059-4b27cd66af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "# from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "# distance = distances.CosineSimilarity()\n",
    "# reducer = reducers.ThresholdReducer(low = 0)\n",
    "# loss_func = losses.TripletMarginLoss(margin = 0.2, distance = distance, reducer = reducer)\n",
    "# mining_func = miners.TripletMarginMiner(margin = 0.2, distance = distance, type_of_triplets = \"semihard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ac8e4e8-af2d-4c2a-93a5-f20f09d4c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "#         data, labels = Variable(data.cuda()), Variable(labels.cuda())\n",
    "#         optimizer.zero_grad()\n",
    "# #         image = torch.rand(1, 3, 8, 100)\n",
    "#         embeddings = model(data.float())\n",
    "# #         embeddings = embeddings.reshape(embeddings.size(0), 181)\n",
    "# #         labels = torch.zeros([64, 1]).cuda()\n",
    "#         print(embeddings.size(), data.size())\n",
    "#         indices_tuple = mining_func(embeddings,data.float())\n",
    "#         loss = loss_func(embeddings, labels, indices_tuple)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 1400 == 0:\n",
    "#             print(\"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(epoch, batch_idx, loss, mining_func.num_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee4cb4c1-ff39-4bdd-a2cb-39db667ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### convenient function from pytorch-metric-learning ###\n",
    "# def get_all_embeddings(dataset, model):\n",
    "#     tester = testers.BaseTester()\n",
    "#     return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "# # def test(testloader, model):\n",
    "# #     model.eval()\n",
    "# #     validation_loss = 0\n",
    "# #     correct = 0\n",
    "# #     total = 0\n",
    "# #     with torch.no_grad():\n",
    "# #         for features, labels in testloader:\n",
    "# #             features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "# #             enn = autoencoder(features.float())\n",
    "# #             auto_outputs = torch.transpose(enn, 2, 3)\n",
    "# #             auto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, 1))\n",
    "# #             loss = criterion(auto_outputs.cuda(), labels.type(torch.LongTensor).cuda())\n",
    "\n",
    "# #             _, pred = torch.max(auto_outputs, 1)\n",
    "# #             total+= labels.reshape(-1).size(0)\n",
    "# #             correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "# #             validation_loss += loss.item()\n",
    "# # #             wandb.log({\"Validation Acc \"+str(val_data):(100*(correct/total)),\"Validation Loss \"+str(val_data):( validation_loss/len(test_dataset_list[val_data]))})\n",
    "# #     print(val_data*10, \"dB SNR is validated\")\n",
    "# # #     vl1[val_data].append((100*(correct/total)))\n",
    "# #     print(\"Validationloss: {}\".format( validation_loss/len(test_dataset_list[val_data])), \" ---- Validation Acc: {}\".format(100*(correct/total)))\n",
    "# # print(\"\\n\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6faa8f4b-5d81-45a9-8a52-2a7e35355ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# device = torch.device(\"cuda\")\n",
    "# autoencoder = Att_R2U()\n",
    "# model = autoencoder.cuda()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ### pytorch-metric-learning stuff ###\n",
    "# distance = distances.CosineSimilarity()\n",
    "# reducer = reducers.ThresholdReducer(low = 0)\n",
    "# loss_func = losses.TripletMarginLoss(margin = 0.2, distance = distance, reducer = reducer) #losses.NTXentLoss(temperature=0.07) \n",
    "# mining_func = miners.TripletMarginMiner(margin = 0.2, distance = distance, type_of_triplets = \"semihard\")\n",
    "# accuracy_calculator = AccuracyCalculator(include = (\"precision_at_1\",), k = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4b0e0e4-b736-40e8-9c00-fd3c4efde593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(0, 100):\n",
    "# #     for data in range(0, len(train_dataset_list)):\n",
    "# #         print(\"Training has started for \", str(data*10), \"dB SNR\" )\n",
    "#     train(model, loss_func, mining_func, device, doa2_train_loader, optimizer, epoch+1)\n",
    "# #     for val_data in range(0, len(test_dataset_list)):\n",
    "# #         print(\"Validation has started for \", str(val_data*10), \"dB SNR\" )\n",
    "# #         test(train_dataset_list[val_data], test_dataset_list[val_data], model, accuracy_calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f13b8d2-9fdd-473e-a342-cd003f1ed4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Att_R2U()\n",
    "\n",
    "num_epochs = 50\n",
    "doa = 100\n",
    "weights_dir = \"./doa_weights/\"\n",
    "\n",
    "# autoencoder = ResNet18()\n",
    "criterion = nn.BCELoss()\n",
    "if ('SNS_DOA_{}_model.pth'.format(doa) in [f for f in listdir(weights_dir) if isfile(join(weights_dir, f))]):\n",
    "    print(\"Pre-trained available for DOA_{}_model.pth\".format(doa))\n",
    "    autoencoder = torch.load(os.path.join(weights_dir, 'SNS_DOA_{}_model.pth'.format(doa)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(torch.cuda.get_device_name(0))\n",
    "\tclassification_model = autoencoder.cuda()\n",
    "\toptimizer = optim.AdamW(classification_model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\tcriterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0faa4a19-7e9a-4c9a-84b6-21109d5ed746",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "loss_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}\n",
    "mae_res = {\n",
    "    \"training\": [],\n",
    "    0: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    40: []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d0a803a-5cc2-4c5e-830d-eac4f7ae6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starts !!!!!!!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\nException raised from createCuDNNHandle at /pytorch/aten/src/ATen/cudnn/Handle.cpp:9 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f813478f1e2 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xfef088 (0x7f80de22e088 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: at::native::getCudnnHandle() + 0x108d (0x7f80de22f96d in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #3: <unknown function> + 0xebcaec (0x7f80de0fbaec in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xeb800e (0x7f80de0f700e in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xeb9bfb (0x7f80de0f8bfb in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::cudnn_convolution_backward_input(c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0xb2 (0x7f80de0f9152 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xf1f35b (0x7f80de15e35b in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf4f178 (0x7f80de18e178 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::cudnn_convolution_backward_input(c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0x1ad (0x7f811903688d in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x223 (0x7f80de0f7823 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf1f445 (0x7f80de15e445 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xf4f1d4 (0x7f80de18e1d4 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #13: at::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x1e2 (0x7f8119045242 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x2ec9c62 (0x7f811ad08c62 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ede224 (0x7f811ad1d224 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: at::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x1e2 (0x7f8119045242 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::generated::CudnnConvolutionBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x258 (0x7f811ab8fc38 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x3375bb7 (0x7f811b1b4bb7 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f811b1b0400 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f811b1b0fa1 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f811b1a9119 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f8135541dea in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #23: <unknown function> + 0xc819d (0x7f814703919d in /home/iiitd/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #24: <unknown function> + 0x76ba (0x7f8149c816ba in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #25: clone + 0x6d (0x7f81499b751d in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b4c1da7a2731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b4c1da7a2731>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#             auto_outputs = torch.reshape(auto_outputs.cuda(), (auto_outputs.shape[0], 181, 3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mlosss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mlosss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#           exp_scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\nException raised from createCuDNNHandle at /pytorch/aten/src/ATen/cudnn/Handle.cpp:9 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f813478f1e2 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xfef088 (0x7f80de22e088 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: at::native::getCudnnHandle() + 0x108d (0x7f80de22f96d in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #3: <unknown function> + 0xebcaec (0x7f80de0fbaec in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xeb800e (0x7f80de0f700e in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xeb9bfb (0x7f80de0f8bfb in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::cudnn_convolution_backward_input(c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0xb2 (0x7f80de0f9152 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xf1f35b (0x7f80de15e35b in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf4f178 (0x7f80de18e178 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::cudnn_convolution_backward_input(c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0x1ad (0x7f811903688d in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x223 (0x7f80de0f7823 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf1f445 (0x7f80de15e445 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xf4f1d4 (0x7f80de18e1d4 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #13: at::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x1e2 (0x7f8119045242 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x2ec9c62 (0x7f811ad08c62 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ede224 (0x7f811ad1d224 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: at::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x1e2 (0x7f8119045242 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::generated::CudnnConvolutionBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x258 (0x7f811ab8fc38 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x3375bb7 (0x7f811b1b4bb7 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f811b1b0400 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f811b1b0fa1 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f811b1a9119 in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f8135541dea in /home/iiitd/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #23: <unknown function> + 0xc819d (0x7f814703919d in /home/iiitd/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #24: <unknown function> + 0x76ba (0x7f8149c816ba in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #25: clone + 0x6d (0x7f81499b751d in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    print(\"Training Starts !!!!!!!\")\n",
    "    best_valid_loss = float('Inf')\n",
    "    for i in range(num_epochs):\n",
    "        training_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        epoch_loss = 0.0\n",
    "        training_mae = 0.0\n",
    "        classification_model.train()\n",
    "        for j,(features, labels) in enumerate(doa2_train_loader, 0):\n",
    "            features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            enn = classification_model(features.float())\n",
    "#             auto_outputs = torch.transpose(enn, 2, 3)\n",
    "#             print(enn.size())\n",
    "#             print(features.size())\n",
    "#             auto_outputs = torch.reshape(auto_outputs.cuda(), (auto_outputs.shape[0], 181, 3))\n",
    "            losss = criterion(enn.cuda(), features.float())\n",
    "            losss.backward()\n",
    "            optimizer.step()\n",
    "#           exp_scheduler.step()\n",
    "            training_loss += losss.item()\n",
    "\n",
    "            _, pred = torch.max(auto_outputs, 1)\n",
    "\n",
    "#             train_total+= labels.reshape(-1).size(0)\n",
    "\n",
    "#             train_correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "\n",
    "            epoch_loss += auto_outputs.shape[0] * losss.item()\n",
    "#             training_mae += torch.abs(pred.reshape(-1).cuda() - labels.reshape(-1)).sum().item()  \n",
    "\n",
    "        loss_res['training'].append(training_loss/len(doa2_train_loader))\n",
    "#         acc_res['training'].append((100*(train_correct/train_total)))\n",
    "#         mae_res['training'].append(training_mae/(128*len(doa2_train_loader)*181))\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}, Training Accuracy: {:.4f}, Training MAE: {}'\n",
    "                      .format(i+1, num_epochs, training_loss/len(doa2_train_loader), (100*(train_correct/train_total)), training_mae/(128*len(doa2_train_loader)*181)))\n",
    "        \n",
    "        # Validation for each SNR value\n",
    "        classification_model.eval()\n",
    "        total_valdation_loss = 0\n",
    "        for val_data in range(0, len(test_dataset_list)):\n",
    "            \n",
    "            validation_loss = 0\n",
    "            validation_acc = 0\n",
    "            validation_mae = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for features, labels in test_dataset_list[val_data]:\n",
    "                    features, labels = Variable(features.cuda()), Variable(labels.cuda())\n",
    "                    enn = classification_model(features.float())\n",
    "#                     auto_outputs = torch.transpose(enn, 2, 3)\n",
    "#                     auto_outputs = torch.reshape(auto_outputs, (auto_outputs.shape[0], 181, 3))\n",
    "                    loss = criterion(enn.cuda(), features.float())\n",
    "\n",
    "                    _, pred = torch.max(auto_outputs, 1)\n",
    "#                     val_total+= labels.reshape(-1).size(0)\n",
    "#                     val_correct+=(pred.reshape(-1).cuda() == labels.reshape(-1)).sum().item()\n",
    "                    validation_loss += loss.item()\n",
    "#                     validation_mae += torch.abs(pred.reshape(-1).cuda() - labels.reshape(-1)).sum().item()\n",
    "\n",
    "                loss_res[10*val_data].append(validation_loss/len(test_dataset_list[val_data]))\n",
    "#                 acc_res[10*val_data].append((100*(val_correct/val_total)))\n",
    "#                 mae_res[10*val_data].append(validation_mae/(128*len(test_dataset_list[val_data])*181))\n",
    "                \n",
    "                print('SNR [{}dB], Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation MAE: {}'\n",
    "                      .format(val_data*10, validation_loss/len(test_dataset_list[val_data]), (100*(val_correct/val_total)), validation_mae/(128*len(test_dataset_list[val_data])*181)))\n",
    "\n",
    "                total_valdation_loss+=validation_loss\n",
    "            torch.save( classification_model, weights_dir+ \"/SNS_DOA_{}_model.pth\".format(doa))\n",
    "            if best_valid_loss > total_valdation_loss:\n",
    "                best_valid_loss = total_valdation_loss \n",
    "                # Saving Best Pre-Trained Model as .pth file\n",
    "                torch.save( classification_model, weights_dir+ \"/DOA_{}_best_model.pth\".format(doa))\n",
    "#         if i%10 == 0:\n",
    "#           ddf = pd.DataFrame(acc_res)\n",
    "#           ddf.to_csv(weights_dir+\"res_DOA_{}_model.csv\".format(doa))\n",
    "        print(\"\\n\")  \n",
    "\n",
    "train()\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae118f-4556-448e-8fe1-0d5368b0c1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
